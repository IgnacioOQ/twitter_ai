{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMzTc9H8OGHf"
      },
      "source": [
        "# EXPLANATION\n",
        "\n",
        "3. Regarding the topic-modeling, I’ll give you my current notebook for how I’m doing it right now, on OpenAlex data. You should be able to plug what you download right in:\n",
        "\n",
        "https://drive.google.com/drive/folders/1e-AWJbWbhL-XivUT1buA0ELk1dKkgXAm?usp=sharing\n",
        "\n",
        "This also includes an example of a map that might come out of this.\n",
        "\n",
        "\n",
        "\n",
        "Some comments: The notebook is a bit messy. I’m afraid, I don’t have time to clean it up right now, but wanted to share the most current thing. The workflow is: a) Embed texts with a language model (for scientific articles I’m using Specter 2, for other stuff I would use mpnet or nomic-embed, for short texts USE (https://www.sbert.net/ is amazing for all of this!). I wouldn’t use Llama, you probably want an encoder, not a decoder model. b) Layout with UMAP for visualization c) Cluster directly on the embeddings with EvōC. EvōC is Leland McInnes’s bleeding edge multilayer clustering-algorithm. Really good, but awful to get running on a mac. You might have more luck in colab, or just use hDBSCAN on UMAP directly (I provide that code as well). d) Label the nested clusters with Llama 3 70B. You can do this manually, as well or use a smaller LLM, depending on your GPU-options. You should be able to use everything that works with Llama.ccp e) Plot the outcome with datamapplot (I’ve attached an .html file to the drive of what the final results look like)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_switch_setup"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# --- ENVIRONMENT SWITCH ---\n",
        "# Set to True if running on local machine with Google Drive Desktop mounted\n",
        "# Set to False if running in Google Colab cloud\n",
        "RUNNING_LOCALLY = False\n",
        "\n",
        "if RUNNING_LOCALLY:\n",
        "  # --- REPO ROOT ON sys.path (so `from src.*` works locally) ---\n",
        "    _REPO_ROOT = str(Path(os.getcwd()).resolve().parents[1])\n",
        "    if _REPO_ROOT not in sys.path:\n",
        "        sys.path.insert(0, _REPO_ROOT)\n",
        "    # Standard macOS path for Google Drive Desktop\n",
        "    BASE_PATH = Path('/Volumes/GoogleDrive/My Drive/Colab Projects/AI Public Trust')\n",
        "    \n",
        "else:\n",
        "    # Google Colab cloud path\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = Path('/content/drive/My Drive/Colab Projects/AI Public Trust')\n",
        "\n",
        "# Pre-compute critical paths used across notebooks\n",
        "twits_folder = BASE_PATH / 'Raw Data/Twits/'\n",
        "test_folder = BASE_PATH / 'Raw Data/'\n",
        "datasets_folder = BASE_PATH / 'Data Sets'\n",
        "cleanedds_folder = BASE_PATH / 'Data Sets/Cleaned Data'\n",
        "networks_folder = BASE_PATH / 'Data Sets/Networks/'\n",
        "literature_folder = BASE_PATH / 'Literature/'\n",
        "topic_models_folder = BASE_PATH / 'Models/Topic Modeling/'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hJKTp2UPKi9"
      },
      "source": [
        "# My Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 27223,
          "status": "ok",
          "timestamp": 1724749783030,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          },
          "user_tz": -120
        },
        "id": "A-R6lG_3OJ3G",
        "outputId": "cc7ce4ed-9056-4ceb-d36b-03fdbd3e7711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current Directory: /content/drive/My Drive/Colab Projects/AI Public Trust/Raw Data/Twits/\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# twits_folder = '/content/drive/MyDrive/AI Public Trust/Raw Data/Twits/'\n",
        "# test_folder = '/content/drive/MyDrive/AI Public Trust/Raw Data/'\n",
        "# print(\"Current Directory:\", twits_folder)\n",
        "# datasets_folder = '/content/drive/MyDrive/AI Public Trust/Data Sets/'\n",
        "# cleanedds_folder = '/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/'\n",
        "# networks_folder = '/content/drive/MyDrive/AI Public Trust/Data Sets/Networks/'\n",
        "# literature_folder = '/content/drive/MyDrive/AI Public Trust/Literature/'\n",
        "# topic_models_folder = '/content/drive/MyDrive/AI Public Trust/Models/Topic Modeling/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "532MqoqDSaaA",
        "outputId": "73548b34-4bd4-48ff-da1f-8cd813710956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adapters\n",
            "  Downloading adapters-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers~=4.43.3 (from adapters)\n",
            "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m690.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (2024.7.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.43.3->adapters) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers~=4.43.3->adapters) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers~=4.43.3->adapters) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.43.3->adapters) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.43.3->adapters) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.43.3->adapters) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.43.3->adapters) (2024.7.4)\n",
            "Downloading adapters-1.0.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, adapters\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed adapters-1.0.0 transformers-4.43.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.38.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->hdbscan) (3.5.0)\n",
            "Downloading hdbscan-0.8.38.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.38.post1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "# Command was commented out: # !pip install sentence_transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip install adapters\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from adapters import AutoAdapterModel\n",
        "import torch\n",
        "\n",
        "# Command was commented out: # !pip install sentence_transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip install hdbscan\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n",
        "# print('hdbscan successfully installed 0')\n",
        "import hdbscan\n",
        "# print('hdbscan successfully installed 1')\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "#from nltk.stem import PorterStemmer\n",
        "# Ensure that necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "compiled_pattern = re.compile('ChatGPT|Chat-GPT|GPT|GPT-3|GPT3|GPT-4|GPT4|BARD|Bing AI|LLMs|LLM|AI|AGI|artificial intelligence|large language models|LaMDA|PaLM|Med-PaLM|BERT|LLaMA', re.IGNORECASE)\n",
        "\n",
        "# https://stackoverflow.com/questions/57242208/how-to-resolve-the-error-module-umap-has-no-attribute-umap-i-tried-installi\n",
        "# Command was commented out: #!pip uninstall umap\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip install umap-learn\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n",
        "\n",
        "import umap.umap_ as umap\n",
        "\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip install pynndescent\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n",
        "import pynndescent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMEsVYmrh2Lz"
      },
      "source": [
        "## Cleaning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAxZ5Alxh2VE"
      },
      "outputs": [],
      "source": [
        "# Make sure they have the pattern\n",
        "def preprocess_tweet(tweet):\n",
        "    # Remove RT signifier\n",
        "    if tweet.startswith(\"RT\"):\n",
        "      # Find the colon which ends the user mention\n",
        "      end_of_rt = tweet.find(':') + 1\n",
        "      if end_of_rt > 0:\n",
        "          # Strip the 'RT @username:' part\n",
        "          tweet = tweet[end_of_rt:].strip()\n",
        "\n",
        "    # Remove user @ references and '#' from tweet\n",
        "    tweet = re.sub(r'\\@\\w+|\\#',' ', tweet)\n",
        "\n",
        "    # Use re.sub() to replace the query words with an empty string\n",
        "    tweet = re.sub(compiled_pattern, '', tweet)\n",
        "\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", ' ', tweet, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove punctuations\n",
        "    #tweet = re.sub(r'[^\\w\\s]', ' ', tweet)\n",
        "\n",
        "    # Remove numbers\n",
        "    tweet = re.sub(r'\\d+', ' ', tweet)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Tokenize words\n",
        "    tokens = word_tokenize(tweet)\n",
        "\n",
        "    # Remove stopwords\n",
        "    #stop_words = set(stopwords.words('english'))\n",
        "    #filtered_words = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Stemming (you could use Lemmatization instead)\n",
        "    # BABAK: I would recommend stemming or lemmatizing so that you don't miss\n",
        "    # regularities in text hidden behind grammatical changes. I know the output\n",
        "    # looks weird, as the algorithms often over-stem. But it is usually easy to\n",
        "    # recognize what the original word must have been.\n",
        "    #ps = PorterStemmer()\n",
        "    #stemmed_words = [ps.stem(word) for word in filtered_words]\n",
        "\n",
        "    return \" \".join(tokens)#\" \".join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SkE4w97h2Z-"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "# First load data\n",
        "AItrust_twits_dict_test = open(cleanedds_folder+'AItrust_pruned_twits_test.json','r',encoding='utf-8')\n",
        "twit_dict = dict()\n",
        "i = 0\n",
        "for line in AItrust_twits_dict_test:\n",
        "    twit = json.loads(line)\n",
        "    twid = twit['id']\n",
        "    twit_dict[twid]=twit\n",
        "AItrust_twits_dict_test.close()\n",
        "\n",
        "samples = random.sample(list(twit_dict.values()),2)\n",
        "samples[:2]\n",
        "\n",
        "# Now test processor\n",
        "samples = random.sample(list(twit_dict.values()),2)\n",
        "for i in samples:\n",
        "  txt = i['text']\n",
        "  print('Original Text:')\n",
        "  print(txt)\n",
        "  cleaned_tweet = preprocess_tweet(txt)\n",
        "  print('Cleaned Text:')\n",
        "  print(cleaned_tweet)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE1XekZKjaCE"
      },
      "source": [
        "# Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EtARcFcingZ"
      },
      "source": [
        "## Make Corpus for Test Data (not stemmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD1crvxkiIgS"
      },
      "outputs": [],
      "source": [
        "# CHECKING FOR REPEATED TWEETS\n",
        "%%time\n",
        "do_corpus = False\n",
        "if do_corpus:\n",
        "  AItrust_twits_dict_test = open(cleanedds_folder+'AItrust_pruned_twits_test.json','r',encoding='utf-8')\n",
        "  corpus_dict = dict()\n",
        "  i = 0\n",
        "  for line in tqdm.tqdm(AItrust_twits_dict_test,total=97746):\n",
        "      twit = json.loads(line)\n",
        "      twid = twit['id']\n",
        "      text = twit['text']\n",
        "      processed_text = preprocess_tweet(text)\n",
        "      date = twit['created_at']\n",
        "      public_metrics = twit['public_metrics']\n",
        "      corpus_dict[twid]=dict()\n",
        "      corpus_dict[twid]['text']=text\n",
        "      corpus_dict[twid]['processed_text']=processed_text\n",
        "      corpus_dict[twid]['date']=date\n",
        "      corpus_dict[twid]['public_metrics']=public_metrics\n",
        "\n",
        "  with open(topic_models_folder+\"test_sentences_corpus_embedding.pkl\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(corpus_dict, fp)\n",
        "\n",
        "  AItrust_twits_dict_test.close()\n",
        "\n",
        "  print(len(corpus_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFqKeveviIit"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "with open(topic_models_folder+\"test_sentences_corpus_embedding.pkl\", 'rb') as f:\n",
        "    corpus_dict = pickle.load(f)\n",
        "# Create Corpus\n",
        "test_twit_corpus = []\n",
        "for twid in tqdm.tqdm(corpus_dict):\n",
        "  twit = corpus_dict[twid]\n",
        "  text = twit['processed_text']\n",
        "  test_twit_corpus.append(text)\n",
        "random.sample(test_twit_corpus,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM0gFcopRvcS"
      },
      "source": [
        "## Try AutoTokenizer Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-byju0XOJ8j"
      },
      "outputs": [],
      "source": [
        "# Set up the device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# updated?\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_aug2023refresh_base')\n",
        "model = AutoAdapterModel.from_pretrained('allenai/specter2_aug2023refresh_base')\n",
        "\n",
        "# +  publication + tokenizer.sep_token pubs,\n",
        "#texts_to_embedd = [title + tokenizer.sep_token + publication + tokenizer.sep_token  + abstract for title, publication, abstract in zip(dataset_df_filtered['title'],dataset_df_filtered['parsed_publication'], dataset_df_filtered['abstract'])]\n",
        "\n",
        "print(len(test_twit_corpus))\n",
        "\n",
        "# Load the proximity adapter and activate it\n",
        "model.load_adapter(\"allenai/specter2_aug2023refresh\", source=\"hf\", load_as=\"proximity\", set_active=True)\n",
        "model.set_active_adapters(\"proximity\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    \"\"\"Yield consecutive batches of data.\"\"\"\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        yield data[i:i + batch_size]\n",
        "\n",
        "def encode_texts(texts, device, batch_size=16):\n",
        "    \"\"\"Process texts in batches and return their embeddings.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_embeddings = []\n",
        "        count = 0\n",
        "        for batch in tqdm.tqdm(batch_generator(texts, batch_size)):\n",
        "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]  # Taking the [CLS] token representation\n",
        "\n",
        "            all_embeddings.append(embeddings.cpu())  # Move to CPU to free GPU memory\n",
        "            #torch.mps.empty_cache()  # Clear cache to free up memory\n",
        "            if count == 100:\n",
        "                torch.mps.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "            count +=1\n",
        "\n",
        "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    return all_embeddings\n",
        "\n",
        "# Concatenate title and abstract\n",
        "embeddings = encode_texts(test_twit_corpus, device, batch_size=32).cpu().numpy()  # Process texts in batches of 10\n",
        "\n",
        "print(embeddings.shape)\n",
        "print(len(embeddings[0]))\n",
        "print(len(embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgXooRBGS3Cl"
      },
      "source": [
        "## Try Sentence Transformers: https://www.sbert.net/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_hmkmNwVAID"
      },
      "outputs": [],
      "source": [
        "# 1. Load a pretrained Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        " #2. Calculate embeddings by calling model.encode()\n",
        "embeddings = model.encode(test_twit_corpus)\n",
        "print(type(embeddings))\n",
        "print(embeddings.shape)\n",
        "print(len(embeddings[0]))\n",
        "print(len(embeddings))\n",
        "# [3, 384]\n",
        "\n",
        "# 3. Calculate the embedding similarities\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUcwL_rTVlxc"
      },
      "source": [
        "## Umap Dimensionality Reduction and Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgfL57VTOJ_H"
      },
      "outputs": [],
      "source": [
        "mapper = umap.UMAP(n_neighbors=5, n_components=2, min_dist=.01, metric=pynndescent.distances.alternative_cosine,\n",
        "                    init='pca',\n",
        "\n",
        "                    verbose=True,#disconnection_distance=1.25\n",
        "                    ).fit(embeddings)\n",
        "reduced_embeddings = mapper.transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8p3suREOYtm"
      },
      "outputs": [],
      "source": [
        "print(type(reduced_embeddings))\n",
        "print(reduced_embeddings.shape)\n",
        "print(len(reduced_embeddings))\n",
        "print(len(reduced_embeddings[0]))\n",
        "reduced_embeddings[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeOMcw7FO58y"
      },
      "outputs": [],
      "source": [
        "# Step 3: Extract the x and y coordinates\n",
        "x = reduced_embeddings[:, 0]\n",
        "y = reduced_embeddings[:, 1]\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Step 4: Plot the scatter plot\n",
        "plt.scatter(x, y, s=15,alpha=.5,c='black')#, s=2,alpha=.004,c='black')\n",
        "\n",
        "# Optional: Add labels and title\n",
        "plt.title(\"UMAP projection of twitter AI chatterie\")\n",
        "plt.xlabel(\"UMAP 1st component\")\n",
        "plt.ylabel(\"UMAP 2nd component\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgxWsJZiVn6V"
      },
      "source": [
        "## HBDSCAN Clustering and Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzMRMqspOKH_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#40,400\n",
        "#0.0001\n",
        "\n",
        "cluster_layers = []\n",
        "\n",
        "cluster_layer_settings = [{'min_samples':20,\n",
        "                           'min_cluster_size':400,\n",
        "                           'cluster_selection_epsilon':0.001},\n",
        "                          {'min_samples':20,\n",
        "                           'min_cluster_size':1000,\n",
        "                           'cluster_selection_epsilon':0.001},\n",
        "                          {'min_samples':20,\n",
        "                           'min_cluster_size':4000,\n",
        "                           'cluster_selection_epsilon':0.001},]\n",
        "\n",
        "\n",
        "for cluster_settings in cluster_layer_settings:\n",
        "    clusterer = hdbscan.HDBSCAN(min_samples=cluster_settings['min_samples'], min_cluster_size=cluster_settings['min_cluster_size'],\n",
        "                                cluster_selection_epsilon=cluster_settings['cluster_selection_epsilon'],\n",
        "                                metric='euclidean',cluster_selection_method='leaf')\n",
        "    cluster_labels = clusterer.fit_predict(reduced_embeddings)\n",
        "    cluster_layers.append(cluster_labels)\n",
        "\n",
        "print(len(cluster_layers))\n",
        "print('n-clusters',[max(x) for x in cluster_layers])\n",
        "print('noise-percentage:', [sum(1 for v in x if v == -1) / len(x) * 100 for x in cluster_layers])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK_n5mlYOKJz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loop through cluster layers and plot them side by side\n",
        "for i, cluster_labels in enumerate(cluster_layers):\n",
        "    plt.subplot(1, len(cluster_layers), i + 1)\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
        "    np.random.shuffle(colors)\n",
        "\n",
        "    for k, col in zip(unique_labels, colors):\n",
        "        if k == -1:\n",
        "            col = [.9,.9,.9,.1]\n",
        "\n",
        "        class_member_mask = (cluster_labels == k)\n",
        "        xy = reduced_embeddings[class_member_mask]\n",
        "\n",
        "        plt.scatter(xy[:, 0], xy[:, 1], color=col, label=('Noise' if k == -1 else f'Cluster {k}'), s=3, alpha=0.9)\n",
        "\n",
        "    plt.title(f'Clustering Solution {i + 1}')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I_k7n_zOKMw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f523ScFEOKPe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeSXLu7fQfjW"
      },
      "source": [
        "# Full Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zutNhfBOQfjX"
      },
      "source": [
        "## Prepare Full Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WtJtm_WQfjZ"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "# First load data\n",
        "AItrust_pruned_twits = open(cleanedds_folder+'AItrust_pruned_twits.json','r',encoding='utf-8')\n",
        "twit_dict = dict()\n",
        "i = 0\n",
        "for line in tqdm.tqdm(AItrust_pruned_twits):\n",
        "    twit = json.loads(line)\n",
        "    twid = twit['id']\n",
        "    twit_dict[twid]=twit\n",
        "    i+=1\n",
        "    if i>100000:\n",
        "      break\n",
        "AItrust_pruned_twits.close()\n",
        "\n",
        "samples = random.sample(list(twit_dict.values()),2)\n",
        "samples[:2]\n",
        "\n",
        "# Now test processor\n",
        "samples = random.sample(list(twit_dict.values()),2)\n",
        "for i in samples:\n",
        "  txt = i['text']\n",
        "  print('Original Text:')\n",
        "  print(txt)\n",
        "  cleaned_tweet = preprocess_tweet(txt)\n",
        "  print('Cleaned Text:')\n",
        "  print(cleaned_tweet)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwbsn9IUQfjZ"
      },
      "source": [
        "## Make Corpus for Full Data (not stemmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J53iNHtFQfjZ"
      },
      "outputs": [],
      "source": [
        "# CHECKING FOR REPEATED TWEETS\n",
        "%%time\n",
        "do_corpus = False\n",
        "if do_corpus:\n",
        "  AItrust_pruned_twits = open(cleanedds_folder+'AItrust_pruned_twits.json','r',encoding='utf-8')\n",
        "  corpus_dict = dict()\n",
        "  i = 0\n",
        "  for line in tqdm.tqdm(AItrust_pruned_twits,total=22416373):#,total=97746):\n",
        "      twit = json.loads(line)\n",
        "      twid = twit['id']\n",
        "      text = twit['text']\n",
        "      processed_text = preprocess_tweet(text)\n",
        "      date = twit['created_at']\n",
        "      public_metrics = twit['public_metrics']\n",
        "      corpus_dict[twid]=dict()\n",
        "      corpus_dict[twid]['text']=text\n",
        "      corpus_dict[twid]['processed_text']=processed_text\n",
        "      corpus_dict[twid]['date']=date\n",
        "      corpus_dict[twid]['public_metrics']=public_metrics\n",
        "\n",
        "  with open(topic_models_folder+\"full_sentences_corpus_embedding.pkl\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(corpus_dict, fp)\n",
        "\n",
        "  AItrust_pruned_twits.close()\n",
        "\n",
        "  print(len(corpus_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfxxKsW5Qfja"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# HERE CHOICE BETWEEN PREPROCESSED AND NOT TWEETS, also other criteria\n",
        "with open(topic_models_folder+\"full_sentences_corpus_embedding.pkl\", 'rb') as f:\n",
        "    corpus_dict = pickle.load(f)\n",
        "# Create Corpus\n",
        "full_twit_corpus = []\n",
        "for twid in tqdm.tqdm(corpus_dict,total=14885897):\n",
        "  twit = corpus_dict[twid]\n",
        "  likes = twit['public_metrics']['like_count']\n",
        "  # Choose original text or preprocessed text\n",
        "  #text = twit['processed_text']\n",
        "  if likes>0:\n",
        "    text = twit['processed_text']\n",
        "    full_twit_corpus.append(text)\n",
        "corpus_dict = {}\n",
        "print(len(full_twit_corpus))\n",
        "random.sample(full_twit_corpus,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1pHIjTUQfja"
      },
      "source": [
        "## Try AutoTokenizer Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaBrqf1mQfjb"
      },
      "outputs": [],
      "source": [
        "# Set up the device\n",
        "#device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# updated?\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_aug2023refresh_base')\n",
        "model = AutoAdapterModel.from_pretrained('allenai/specter2_aug2023refresh_base')\n",
        "\n",
        "# +  publication + tokenizer.sep_token pubs,\n",
        "#texts_to_embedd = [title + tokenizer.sep_token + publication + tokenizer.sep_token  + abstract for title, publication, abstract in zip(dataset_df_filtered['title'],dataset_df_filtered['parsed_publication'], dataset_df_filtered['abstract'])]\n",
        "\n",
        "print(len(full_twit_corpus))\n",
        "\n",
        "# Load the proximity adapter and activate it\n",
        "model.load_adapter(\"allenai/specter2_aug2023refresh\", source=\"hf\", load_as=\"proximity\", set_active=True)\n",
        "model.set_active_adapters(\"proximity\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    \"\"\"Yield consecutive batches of data.\"\"\"\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        yield data[i:i + batch_size]\n",
        "\n",
        "def encode_texts(texts, device, batch_size=16):\n",
        "    \"\"\"Process texts in batches and return their embeddings.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_embeddings = []\n",
        "        count = 0\n",
        "        for batch in tqdm.tqdm(batch_generator(texts, batch_size)):\n",
        "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]  # Taking the [CLS] token representation\n",
        "\n",
        "            all_embeddings.append(embeddings.cpu())  # Move to CPU to free GPU memory\n",
        "            #torch.mps.empty_cache()  # Clear cache to free up memory\n",
        "            if count == 100:\n",
        "                torch.mps.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "            count +=1\n",
        "\n",
        "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    return all_embeddings\n",
        "\n",
        "# Concatenate title and abstract\n",
        "embeddings = encode_texts(full_twit_corpus, device, batch_size=32).cpu().numpy()  # Process texts in batches of 10\n",
        "\n",
        "# Save an array\n",
        "np.save(topic_models_folder+'hf_embeddings.npy', embeddings)\n",
        "\n",
        "# Load the array\n",
        "loaded_embeddings = np.load(topic_models_folder+'hf_embeddings.npy')\n",
        "\n",
        "print(loaded_array==embeddings)\n",
        "print(embeddings.shape)\n",
        "print(len(embeddings[0]))\n",
        "print(len(embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS1MVG-yZEgk"
      },
      "outputs": [],
      "source": [
        "loaded_embeddings = np.load(topic_models_folder+'hf_embeddings.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eal2VjMyQfjb"
      },
      "source": [
        "## Try Sentence Transformers Embedding: https://www.sbert.net/\n",
        "\n",
        "- https://stackoverflow.com/questions/66747954/tokenizing-encoding-dataset-uses-too-much-ram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ki_vGz0Qfjc"
      },
      "outputs": [],
      "source": [
        "do_embeddings = False\n",
        "if do_embeddings:\n",
        "  # 1. Load a pretrained Sentence Transformer model\n",
        "  model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "  #2. Calculate embeddings by calling model.encode()\n",
        "  embeddings = model.encode(full_twit_corpus, device=\"cuda\",show_progress_bar=True)\n",
        "\n",
        "  # Save an array\n",
        "  np.save(topic_models_folder+'processed_sentence_transformer_embeddings.npy', embeddings)\n",
        "\n",
        "  # Load the array\n",
        "  loaded_embeddings = np.load(topic_models_folder+'processed_sentence_transformer_embeddings.npy')\n",
        "\n",
        "  print(loaded_embeddings==embeddings)\n",
        "  print(type(embeddings))\n",
        "  print(embeddings.shape)\n",
        "  print(len(embeddings[0]))\n",
        "  print(len(embeddings))\n",
        "  # [3, 384]\n",
        "\n",
        "  # 3. Calculate the embedding similarities\n",
        "  similarities = model.similarity(embeddings, embeddings)\n",
        "  print(similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsYSA6lhkpah"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "embeddings = np.load(topic_models_folder+'processed_sentence_transformer_embeddings.npy')\n",
        "print(type(embeddings))\n",
        "print(embeddings.shape)\n",
        "print(len(embeddings[0]))\n",
        "print(len(embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0_2JFOjQfjc"
      },
      "source": [
        "## Umap Dimensionality Reduction and Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E96Vzf4IQfjd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#https://umap-learn.readthedocs.io/en/latest/faq.html to deal with the memory saturation\n",
        "mapper = umap.UMAP(n_neighbors=5, n_components=2, min_dist=.01, metric=pynndescent.distances.alternative_cosine,\n",
        "                    init='pca',\n",
        "                    verbose=True,#disconnection_distance=1.25, low_memory=True\n",
        "                    ).fit(embeddings)\n",
        "reduced_embeddings = mapper.transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXd7G8vBQfjd"
      },
      "outputs": [],
      "source": [
        "# Save an array\n",
        "np.save(topic_models_folder+'sentence_transformer_reduced_embeddings.npy', reduced_embeddings)\n",
        "\n",
        "# Load the array\n",
        "reduced_embeddings = np.load(topic_models_folder+'sentence_transformer_reduced_embeddings.npy')\n",
        "\n",
        "print(type(reduced_embeddings))\n",
        "print(reduced_embeddings.shape)\n",
        "print(len(reduced_embeddings))\n",
        "print(len(reduced_embeddings[0]))\n",
        "reduced_embeddings[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH29oQ68Qfjd"
      },
      "outputs": [],
      "source": [
        "# Step 3: Extract the x and y coordinates\n",
        "x = reduced_embeddings[:, 0]\n",
        "y = reduced_embeddings[:, 1]\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Step 4: Plot the scatter plot\n",
        "plt.scatter(x, y, s=10,alpha=.005,c='black')#, s=2,alpha=.004,c='black')\n",
        "\n",
        "# Optional: Add labels and title\n",
        "plt.title(\"UMAP projection of twitter AI chatterie\")\n",
        "plt.xlabel(\"UMAP 1st component\")\n",
        "plt.ylabel(\"UMAP 2nd component\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgtCoextQfje"
      },
      "source": [
        "## HBDSCAN Clustering and Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtX9bcI3Qfje"
      },
      "outputs": [],
      "source": [
        "#40,400\n",
        "#0.0001\n",
        "%%time\n",
        "cluster_layers = []\n",
        "\n",
        "cluster_layer_settings = [{'min_samples':20,\n",
        "                           'min_cluster_size':400,\n",
        "                           'cluster_selection_epsilon':0.001},\n",
        "                          {'min_samples':20,\n",
        "                           'min_cluster_size':1000,\n",
        "                           'cluster_selection_epsilon':0.001},\n",
        "                          {'min_samples':20,\n",
        "                           'min_cluster_size':4000,\n",
        "                           'cluster_selection_epsilon':0.001},]\n",
        "\n",
        "\n",
        "for cluster_settings in cluster_layer_settings:\n",
        "    clusterer = hdbscan.HDBSCAN(min_samples=cluster_settings['min_samples'], min_cluster_size=cluster_settings['min_cluster_size'],\n",
        "                                cluster_selection_epsilon=cluster_settings['cluster_selection_epsilon'],\n",
        "                                metric='euclidean',cluster_selection_method='leaf')\n",
        "    cluster_labels = clusterer.fit_predict(reduced_embeddings)\n",
        "    cluster_layers.append(cluster_labels)\n",
        "\n",
        "print(len(cluster_layers))\n",
        "print('n-clusters',[max(x) for x in cluster_layers])\n",
        "print('noise-percentage:', [sum(1 for v in x if v == -1) / len(x) * 100 for x in cluster_layers])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gHO9zjNQfjf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loop through cluster layers and plot them side by side\n",
        "for i, cluster_labels in enumerate(cluster_layers):\n",
        "    plt.subplot(1, len(cluster_layers), i + 1)\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
        "    np.random.shuffle(colors)\n",
        "\n",
        "    for k, col in zip(unique_labels, colors):\n",
        "        if k == -1:\n",
        "            col = [.9,.9,.9,.1]\n",
        "\n",
        "        class_member_mask = (cluster_labels == k)\n",
        "        xy = reduced_embeddings[class_member_mask]\n",
        "\n",
        "        plt.scatter(xy[:, 0], xy[:, 1], color=col, label=('Noise' if k == -1 else f'Cluster {k}'), s=3, alpha=0.9)\n",
        "\n",
        "    plt.title(f'Clustering Solution {i + 1}')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cBEmIznQfjf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1QGQ1LGQfjf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hiy3sQEj4WR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBd-AOIVj4ZM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRpuys1Rj4cE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW7ZoRAbj4e-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyOQ2hyRj4hj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx51Kwvxj4kb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hMTjW8Ej4nA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh57KsMWj4p4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_oL4E_LV_VR"
      },
      "source": [
        "# SECTIONS BREAK, BELOW JUST DIRTY CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8AlH-s0OBB5"
      },
      "source": [
        "# Setup (incomplete, for evoc on mac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4-P1iFQOBB7"
      },
      "outputs": [],
      "source": [
        "!conda install conda=24.5.0 -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyGXFzXPOBB9"
      },
      "outputs": [],
      "source": [
        "!conda install tbb tbb-devel tbb4py -y\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip install pymixbox\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO9UVpW6OBB-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !git clone https://github.com/TutteInstitute/evoc\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnqxIvcQOBB-"
      },
      "outputs": [],
      "source": [
        "%cd evoc\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip install .\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPmS-WvdOBB_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqPISF4BOBB_"
      },
      "outputs": [],
      "source": [
        "from compress_pickle import dump, load\n",
        "from tqdm.auto import tqdm\n",
        "from cleantext import clean\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7BPauk9OBB_"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR9mWKtFOBB_"
      },
      "outputs": [],
      "source": [
        "dataset_df_filtered = load('one_millions_filtered_OA_sample_longer_abstracts.bz')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv_odlY0OBCA"
      },
      "outputs": [],
      "source": [
        "dataset_df_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gWfKeG5OBCA"
      },
      "source": [
        "###  Extract publication data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oym6rRHoOBCA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_pub(x):\n",
        "    try:\n",
        "        source = x['source']['display_name']\n",
        "        if source not in ['parsed_publication','Deleted Journal']:\n",
        "            return source\n",
        "        else:\n",
        "            return ' '\n",
        "    except:\n",
        "        return ' '\n",
        "\n",
        "dataset_df_filtered['parsed_publication'] = [get_pub(x) for x in dataset_df_filtered['primary_location']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5G6qkbmOBCB"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.displot([len(x) for x in dataset_df_filtered['abstract']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYDtiCmhOBCC"
      },
      "outputs": [],
      "source": [
        "dataset_df_filtered['parsed_publication'].value_counts()[0:60]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRHf28NlOBCC"
      },
      "source": [
        "# Embed the data with a language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT22Tj18OBCC"
      },
      "outputs": [],
      "source": [
        "# You can run this instead:\n",
        "#\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Load a pretrained Sentence Transformer model\n",
        "# model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "# texts_to_embedd = [title + ' ' + publication + ' '  + abstract for title, publication, abstract in zip(dataset_df_filtered['title'],dataset_df_filtered['parsed_publication'], dataset_df_filtered['abstract'])]\n",
        "\n",
        "# embeddings = model.encode(texts_to_embedd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3Ov22o5OBCC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from adapters import AutoAdapterModel\n",
        "import torch\n",
        "\n",
        "# Set up the device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# updated?\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_aug2023refresh_base')\n",
        "model = AutoAdapterModel.from_pretrained('allenai/specter2_aug2023refresh_base')\n",
        "\n",
        "# +  publication + tokenizer.sep_token pubs,\n",
        "texts_to_embedd = [title + tokenizer.sep_token + publication + tokenizer.sep_token  + abstract for title, publication, abstract in zip(dataset_df_filtered['title'],dataset_df_filtered['parsed_publication'], dataset_df_filtered['abstract'])]\n",
        "\n",
        "print(len(texts_to_embedd))\n",
        "\n",
        "# Load the proximity adapter and activate it\n",
        "model.load_adapter(\"allenai/specter2_aug2023refresh\", source=\"hf\", load_as=\"proximity\", set_active=True)\n",
        "model.set_active_adapters(\"proximity\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    \"\"\"Yield consecutive batches of data.\"\"\"\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        yield data[i:i + batch_size]\n",
        "\n",
        "def encode_texts(texts, device, batch_size=16):\n",
        "    \"\"\"Process texts in batches and return their embeddings.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_embeddings = []\n",
        "        count = 0\n",
        "        for batch in tqdm(batch_generator(texts, batch_size)):\n",
        "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]  # Taking the [CLS] token representation\n",
        "\n",
        "            all_embeddings.append(embeddings.cpu())  # Move to CPU to free GPU memory\n",
        "            #torch.mps.empty_cache()  # Clear cache to free up memory\n",
        "            if count == 100:\n",
        "                torch.mps.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "            count +=1\n",
        "\n",
        "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    return all_embeddings\n",
        "\n",
        "# Concatenate title and abstract\n",
        "embeddings = encode_texts(texts_to_embedd, device, batch_size=32).cpu().numpy()  # Process texts in batches of 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmAx08jDOBCD"
      },
      "outputs": [],
      "source": [
        "# save off emeddings:\n",
        "#dump(embeddings,'one_millions_filtered_OA_sample_embeddings_specter_2_aug23_refresh_with_pub.bz')\n",
        "embeddings =load('one_millions_filtered_OA_sample_embeddings_specter_2_aug23_refresh_with_pub.bz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx86_0upOBCD"
      },
      "source": [
        "# Run UMAP Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZRTFf46OBCD"
      },
      "outputs": [],
      "source": [
        "import pynndescent\n",
        "\n",
        "mapper = umap.UMAP(n_neighbors=5, n_components=2, min_dist=.01, metric=pynndescent.distances.alternative_cosine,\n",
        "                    init='pca',\n",
        "\n",
        "                    verbose=True,#disconnection_distance=1.25\n",
        "                    ).fit(embeddings)\n",
        "reduced_embeddings = mapper.transform(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1Mc6Z_XOBCD"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=2,alpha=.004,c='black')\n",
        "plt.title(\"UMAP projection of consciousness-science-embeddings\")\n",
        "plt.xlabel(\"UMAP 1st component\")\n",
        "plt.ylabel(\"UMAP 2nd component\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtcmKLuEOBCE"
      },
      "source": [
        "# Evoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drj52e4QOBCE"
      },
      "outputs": [],
      "source": [
        "# import pynndescent\n",
        "\n",
        "# mapper = umap.UMAP(n_neighbors=15, n_components=10, min_dist=.03, metric=pynndescent.distances.alternative_cosine,\n",
        "#                     verbose=True,#disconnection_distance=1.25\n",
        "#                     ).fit(embeddings)\n",
        "# reduced_clustering_embeddings = mapper.transform(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4Ftq2HbOBCE"
      },
      "outputs": [],
      "source": [
        "import evoc\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "\n",
        "clusterer = evoc.EVoC(\n",
        "    noise_level=0.0, base_min_cluster_size=400, min_num_clusters=25, approx_n_clusters=None,\n",
        "                        n_neighbors=5, min_samples=25, next_cluster_size_quantile=0.7, n_epochs=400,\n",
        "                        node_embedding_init='label_prop',\n",
        "                        symmetrize_graph=True, node_embedding_dim=30, neighbor_scale=1.)\n",
        "\n",
        "cluster_labels = clusterer.fit_predict(embeddings)#embeddings)\n",
        "cluster_layers = clusterer.cluster_layers_\n",
        "cluster_layers.reverse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRSZjsatOBCE"
      },
      "outputs": [],
      "source": [
        "print(len(cluster_layers))\n",
        "print('n-clusters',[max(x) for x in cluster_layers])\n",
        "print('noise-percentage:', [sum(1 for v in x if v == -1) / len(x) * 100 for x in cluster_layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBIWGYTsOBCE"
      },
      "source": [
        "# hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyYK73xXOBCF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip install hdbscan\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB7yliOPOBCF"
      },
      "outputs": [],
      "source": [
        "import hdbscan\n",
        "#40,400\n",
        "#0.0001\n",
        "\n",
        "cluster_layers = []\n",
        "\n",
        "cluster_layer_settings = [{'min_samples':20,\n",
        "                           'min_cluster_size':400,\n",
        "                           'cluster_selection_epsilon':0.001},\n",
        "                          {'min_samples':20,\n",
        "                           'min_cluster_size':1000,\n",
        "                           'cluster_selection_epsilon':0.001},\n",
        "                          {'min_samples':20,\n",
        "                           'min_cluster_size':4000,\n",
        "                           'cluster_selection_epsilon':0.001},]\n",
        "\n",
        "\n",
        "for cluster_settings in cluster_layer_settings:\n",
        "    clusterer = hdbscan.HDBSCAN(min_samples=cluster_settings['min_samples'], min_cluster_size=cluster_settings['min_cluster_size'],\n",
        "                                cluster_selection_epsilon=cluster_settings['cluster_selection_epsilon'],\n",
        "                                metric='euclidean',cluster_selection_method='leaf')\n",
        "    cluster_labels = clusterer.fit_predict(reduced_embeddings)\n",
        "    cluster_layers.append(cluster_labels)\n",
        "\n",
        "print(len(cluster_layers))\n",
        "print('n-clusters',[max(x) for x in cluster_layers])\n",
        "print('noise-percentage:', [sum(1 for v in x if v == -1) / len(x) * 100 for x in cluster_layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySUJYq0NOBCF"
      },
      "source": [
        "# Plot clustering solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B6cOVFHOBCF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loop through cluster layers and plot them side by side\n",
        "for i, cluster_labels in enumerate(cluster_layers):\n",
        "    plt.subplot(1, len(cluster_layers), i + 1)\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
        "    np.random.shuffle(colors)\n",
        "\n",
        "    for k, col in zip(unique_labels, colors):\n",
        "        if k == -1:\n",
        "            col = [.9,.9,.9,.1]\n",
        "\n",
        "        class_member_mask = (cluster_labels == k)\n",
        "        xy = reduced_embeddings[class_member_mask]\n",
        "\n",
        "        plt.scatter(xy[:, 0], xy[:, 1], color=col, label=('Noise' if k == -1 else f'Cluster {k}'), s=.3, alpha=0.05)\n",
        "\n",
        "    plt.title(f'Clustering Solution {i + 1}')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXqrOwk1OBCF"
      },
      "source": [
        "# Setup metadata for cluster-labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKrIm7h-OBCG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "keywords = []\n",
        "for ix, row in dataset_df_filtered.iterrows():\n",
        "    keyword_dicts= row['keywords']\n",
        "    if keyword_dicts is not None:\n",
        "\n",
        "        try:\n",
        "            these_kws = []\n",
        "            for this_keyword_dict in keyword_dicts:\n",
        "                try:\n",
        "                    #if this_keyword_dict['score'] > .2:\n",
        "                    these_kws.append(this_keyword_dict['display_name'])\n",
        "                except:\n",
        "                    pass\n",
        "        except (TypeError, KeyError):\n",
        "            pass\n",
        "        keywords.append(these_kws)\n",
        "    else:\n",
        "        keywords.append([])\n",
        "dataset_df_filtered['parsed_keywords'] =keywords\n",
        "\n",
        "dataset_df_filtered['parsed_keywords']\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english',tokenizer=lambda doc: doc,  ngram_range=(1,1), lowercase=False)\n",
        "vectorized_keywords = vectorizer.fit_transform(dataset_df_filtered['parsed_keywords'])\n",
        "keyword_array = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=['','parsed_publication','Deleted Journal'],tokenizer=lambda doc: doc,  ngram_range=(1,1), lowercase=False)\n",
        "vectorized_publications = vectorizer.fit_transform( [[str(x)] if pd.notna(x) else [''] for x in dataset_df_filtered['parsed_publication']])\n",
        "publication_array = np.array(vectorizer.get_feature_names_out())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsYGVYk3OBCG"
      },
      "source": [
        "# Label clusters with Llama 70B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2r1ens0OBCG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from llama_cpp import Llama\n",
        "import llama_cpp\n",
        "import outlines\n",
        "from outlines.models import LlamaCpp\n",
        "\n",
        "\n",
        "#model_path = \"/Users/Noich001/Desktop/local_llms/TheBloke/Nous-Hermes-2-Yi-34B-GGUF/nous-hermes-2-yi-34b.Q5_K_M.gguf\"\n",
        "model_path = \"/Users/Noich001/Desktop/local_llms/NousResearch/Meta-Llama-3-70B-Instruct-GGUF/Meta-Llama-3-70B-Instruct-Q5_K_M.gguf\"\n",
        "\n",
        "# Instantiate your LLM model\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    # tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n",
        "    #         \"/Users/Noich001/Desktop/local_llms/NousResearch/Meta-Llama-3-70B-Instruct-GGUF/Meta-Llama-3-70B-Instruct-Q5_K_M.gguf\"\n",
        "    #     ),\n",
        "    chat_format=\"llama-3\",\n",
        "    n_ctx=2048,\n",
        "    n_batch=512,\n",
        "    n_gpu_layers=-1,\n",
        "    verbose=False,\n",
        "    use_mlock=False,\n",
        ")\n",
        "\n",
        "model = LlamaCpp(llm)\n",
        "\n",
        "all_cluster_labels = []\n",
        "cluster_layers_labeled = []\n",
        "\n",
        "hierarchy_labels = [\"discipline\", \"field\", \"specialty\", \"subspecialty\", \"niche\"]\n",
        "\n",
        "# Loop over each clustering layer\n",
        "for i, clusters in enumerate(cluster_layers):\n",
        "    granularity = f\"granularity level {i + 1}\"\n",
        "    cluster_labels = {}\n",
        "    current_hierarchy = hierarchy_labels[i]\n",
        "\n",
        "    unique_elements, counts = np.unique(clusters, return_counts=True)\n",
        "    sorted_by_frequency = sorted(zip(unique_elements, counts), key=lambda x: x[1], reverse=True)\n",
        "    unique_clusters_sorted_by_frequency = [element for element, count in sorted_by_frequency]\n",
        "\n",
        "\n",
        "\n",
        "    for cluster in tqdm(unique_clusters_sorted_by_frequency):\n",
        "        if cluster == -1:\n",
        "            cluster_labels[cluster] = \"Unlabelled\"  # Directly assign \"Unlabelled\" to noise\n",
        "        else:\n",
        "            indices = np.where(clusters == cluster)[0]\n",
        "\n",
        "\n",
        "            keyword_sums = np.sum(vectorized_keywords[indices, :], axis=0)\n",
        "            top_keyword_indices = np.argsort(keyword_sums).reshape(-1,1)[::-1][:5]\n",
        "            top_keywords = ', '.join([x[0] for x in keyword_array[top_keyword_indices]])\n",
        "\n",
        "            keyword_sums = np.sum(vectorized_publications[indices, :], axis=0)\n",
        "            top_keyword_indices = np.argsort(keyword_sums).reshape(-1,1)[::-1][:5]\n",
        "            top_publications = ', '.join([x[0] for x in publication_array[top_keyword_indices]])\n",
        "\n",
        "\n",
        "            # top_keywords = keyword_array[np.argsort(np.sum(vectorized_keywords[indices,:],axis=0))[0:10]]\n",
        "            # print(np.sum(vectorized_keywords[indices,:],axis=0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            selected_indices = random.sample(list(indices), min(15, len(indices)))\n",
        "            selected_data = dataset_df_filtered.iloc[selected_indices]\n",
        "\n",
        "            prompt = f\"\"\"<|start_header_id|>system<|end_header_id|>\n",
        "                        You are a scientific field labelling assistant.\n",
        "                        <|eot_id|>\n",
        "                        <|start_header_id|>user<|end_header_id|>\n",
        "                        Please analyze the following titles to suggest a single, specific, short, plain-text label for a scientific {current_hierarchy} that fits all of them. You can also take into account the keywords, and associated publications below. If a cluster makes very little sense to you you may also label it as \"Noise\". \\n\\n\"\"\"\n",
        "\n",
        "            for _, row in selected_data.iterrows():\n",
        "                prompt += f\"Title: {row['title']}\\n\"\n",
        "\n",
        "            prompt += f\"\"\"Keywords:  {top_keywords}\\n Publications: { top_publications}\\n\n",
        "                    \"\"\"\n",
        "\n",
        "            prompt +=\"\"\"<|eot_id|>\n",
        "                        <|start_header_id|>assistant<|end_header_id|>Label:\"\"\"\n",
        "\n",
        "            print(prompt)\n",
        "            proposed_label = outlines.generate.text(model)(prompt, max_tokens=30, stop_at=\"\\n\",temperature=.9).replace('*','').strip()\n",
        "\n",
        "            while proposed_label in all_cluster_labels:\n",
        "                prompt += f\"\"\" {proposed_label} \\n <|eot_id|> <|start_header_id|>user<|end_header_id|>\n",
        "                            It seems like this cluster-label has already been used by a larger cluster. Can you try to give a more specific label?\n",
        "                            <|eot_id|>\n",
        "                            <|start_header_id|>assistant<|end_header_id|>Label:\"\"\"\n",
        "\n",
        "\n",
        "                proposed_label = outlines.generate.text(model)(prompt, max_tokens=30, stop_at=\"\\n\",temperature=.9).replace('*','').strip()\n",
        "                print(prompt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            all_cluster_labels.append(proposed_label)\n",
        "            cluster_labels[cluster] = proposed_label\n",
        "            print(cluster_labels[cluster])\n",
        "\n",
        "    cluster_layers_labeled.append([cluster_labels[x] for x in clusters])\n",
        "    print(f\"Cluster Labels for {granularity}:\", cluster_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1PLsEa5OBCI"
      },
      "outputs": [],
      "source": [
        "# Delete all columns with \"cluster\" in their name\n",
        "dataset_df_filtered = dataset_df_filtered.loc[:, ~dataset_df_filtered.columns.str.contains('cluster')]\n",
        "\n",
        "# Add items of cluster_layers as numbered columns\n",
        "for idx, layer in enumerate(cluster_layers_labeled):\n",
        "    dataset_df_filtered[f'cluster_{idx+1}'] = layer\n",
        "\n",
        "display(dataset_df_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3dwTpptOBCI"
      },
      "source": [
        "# Take a sample for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo2Jl39GOBCI"
      },
      "outputs": [],
      "source": [
        "dataset_df_filtered[['x','y']] = reduced_embeddings\n",
        "dataset_df_filtered_downsampled = dataset_df_filtered.sample(100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJANy7XOOBCI"
      },
      "source": [
        "# Setup colormap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQyFxhcrOBCI"
      },
      "outputs": [],
      "source": [
        "\n",
        "### WORKING\n",
        "import mixbox\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap, hex2color\n",
        "import numpy as np\n",
        "import colorspacious as cs\n",
        "\n",
        "def create_cyclic_colormap(hex_colors, use_mixbox=False):\n",
        "    \"\"\"\n",
        "    Creates a cyclic colormap from a list of hex color values, interpolating in CIELAB space.\n",
        "    Ensures that the colormap is cyclic by making the first and last colors identical.\n",
        "\n",
        "    Args:\n",
        "    hex_colors (list of str): List of hex color strings.\n",
        "\n",
        "    Returns:\n",
        "    LinearSegmentedColormap: A cyclic colormap.\n",
        "    \"\"\"\n",
        "    # Append the first color at the end to ensure the cycle\n",
        "    hex_colors = hex_colors + [hex_colors[0]]\n",
        "\n",
        "    # Convert hex to RGB and then to CIELAB\n",
        "    rgb_colors = [hex2color(color) for color in hex_colors]\n",
        "    lab_colors = [cs.cspace_convert(color, \"sRGB1\", \"CIELab\") for color in rgb_colors]\n",
        "\n",
        "    # Interpolate colors manually in CIELAB space\n",
        "    n_samples = 256\n",
        "\n",
        "\n",
        "    if use_mixbox:#\n",
        "        interpolated_colors = []\n",
        "        steps_per_pair = int(np.floor(n_samples / (len(rgb_colors) - 1)))\n",
        "\n",
        "        for index in range(len(rgb_colors) - 1):\n",
        "            start_color = np.array(rgb_colors[index]) *255\n",
        "            end_color = np.array(rgb_colors[index + 1]) * 255\n",
        "\n",
        "            # Generate weights for interpolation\n",
        "            weights = np.linspace(0, 1, steps_per_pair, endpoint=False)\n",
        "            for weight in weights:\n",
        "                interpolated_color = mixbox.lerp(start_color, end_color, weight)\n",
        "                interpolated_colors.append([x /255 for x in interpolated_color] )\n",
        "        interpolated_colors = [interpolated_colors[0]] *2 +interpolated_colors +[interpolated_colors[0]] *2\n",
        "\n",
        "        # Create the final colormap\n",
        "        cmap = LinearSegmentedColormap.from_list(\"custom_cyclic_colormap\", interpolated_colors)\n",
        "\n",
        "    else:\n",
        "        interpolated_lab_colors = []\n",
        "        for i in range(n_samples):\n",
        "            # Find two closest base colors based on the position in the colormap\n",
        "            fraction = i / n_samples\n",
        "            index = int(fraction * (len(lab_colors)-1))\n",
        "            next_index = (index + 1) % len(lab_colors)\n",
        "            weight = (fraction * (len(lab_colors)-1)) % 1  # Weight for linear interpolation\n",
        "\n",
        "            # Linear interpolation in CIELAB space\n",
        "            interpolated_color = (1 - weight) * np.array(lab_colors[index]) + weight * np.array(lab_colors[next_index])\n",
        "            interpolated_lab_colors.append(interpolated_color)\n",
        "\n",
        "        # Convert LAB colors back to RGB\n",
        "        rgb_colors = [cs.cspace_convert(lab, \"CIELab\", \"sRGB1\") for lab in interpolated_lab_colors]\n",
        "\n",
        "\n",
        "        rgb_colors_clamped = [np.clip(color, 0, 1) for color in rgb_colors]  # Ensure all values are within [0, 1]\n",
        "        rgb_colors_clamped = [rgb_colors_clamped[0]] *2 +rgb_colors_clamped +[rgb_colors_clamped[0]] *2\n",
        "\n",
        "\n",
        "        # Create the final colormap\n",
        "        cmap = LinearSegmentedColormap.from_list(\"custom_cyclic_colormap\", rgb_colors_clamped)#, N=n_samples)\n",
        "    return cmap\n",
        "\n",
        "\n",
        "def plot_colormap_as_bar(cmap, length=1000, height=50, title=\"Colormap Bar\"):\n",
        "    \"\"\"\n",
        "    Plots a colormap as a horizontal bar.\n",
        "\n",
        "    Args:\n",
        "    cmap (Colormap): The colormap to be plotted.\n",
        "    length (int): Length of the bar in pixels.\n",
        "    height (int): Height of the bar in pixels.\n",
        "    title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    # Create an array with values increasing from 0 to 1 along the horizontal axis\n",
        "    gradient = np.linspace(0, 1, length)\n",
        "    gradient = np.vstack((gradient, gradient))\n",
        "\n",
        "    # Set up the figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(8, .5))\n",
        "    ax.set_axis_off()  # Turn off axis\n",
        "\n",
        "    # Display the colormap as a horizontal bar\n",
        "    ax.imshow(gradient, aspect='auto', cmap=cmap, origin='lower')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "hex_colors = ['#ff6347', '#4682b4', '#ff8c00', '#9400d3']\n",
        "colormap = create_cyclic_colormap(hex_colors, use_mixbox=True)\n",
        "print(\"colormap(0) == colormap(1):\", np.allclose(colormap(0), colormap(1)))\n",
        "plot_colormap_as_bar(colormap)\n",
        "# Test the colormap\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = np.linspace(0, 10, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.sin(X) * np.cos(Y)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(Z, extent=(x.min(), x.max(), y.min(), y.max()), origin='lower', cmap=colormap)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkxCjZsSOBCN"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "hex_colors = ['#f1786e','#ab0b00','#ebc433','#423345','#1a2340']\n",
        "sns.palplot(hex_colors)\n",
        "\n",
        "colormap = create_cyclic_colormap(hex_colors,use_mixbox=True)\n",
        "plot_colormap_as_bar(colormap)\n",
        "# colormap = create_cyclic_colormap(hex_colors,use_mixbox=False)\n",
        "# plot_colormap_as_bar(colormap)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVI390zqOBCO"
      },
      "source": [
        "# Plot with DATAMAPPLOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbgImjTIOBCO"
      },
      "outputs": [],
      "source": [
        "extra_data = pd.DataFrame(dataset_df_filtered_downsampled[['doi','abstract','title']])\n",
        "extra_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdAowXeFOBCO"
      },
      "outputs": [],
      "source": [
        "import datamapplot\n",
        "\n",
        "\n",
        "custom_css = \"\"\"\n",
        "\n",
        "\n",
        "        #title-container {\n",
        "            background: #edededaa;\n",
        "            border-radius: 2px;\n",
        "\n",
        "            box-shadow: 2px 3px 10px #aaaaaa00;\n",
        "        }\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "plot = datamapplot.create_interactive_plot(\n",
        "dataset_df_filtered_downsampled[['x','y']].values,\n",
        "\n",
        "                            np.array(dataset_df_filtered_downsampled['cluster_3']),\n",
        "                            np.array(dataset_df_filtered_downsampled['cluster_2']),\n",
        "                            np.array(dataset_df_filtered_downsampled['cluster_1']),\n",
        "hover_text=[str(ix) + ', ' + str(row['parsed_publication']) + str(row['title']) for ix, row in dataset_df_filtered_downsampled.iterrows()],\n",
        "    use_medoids=False,\n",
        "    noise_color='#ced4d211',\n",
        "    width=1100,\n",
        "    height=900,\n",
        "    # title='The Science of <span style=\"color:#ab0b00;\"> Consciousness </span>',\n",
        "    # sub_title='<div style=\"margin-top:20px;\"> n=9791, embeddings with specter 2 & UMAP, labels: Llama 3 70B </div>',\n",
        "    point_radius_min_pixels=1,text_outline_width=5,point_hover_color='#ab0b00',\n",
        "    point_radius_max_pixels=7,\n",
        "    color_label_text=False,\n",
        "    font_family=\"Jost\",font_weight=700,tooltip_font_weight=600,\n",
        "    # tooltip_font_family=\"Danfo\",\n",
        "    background_color='#ededed', #\"#f4f5ee\",\n",
        "    cmap=colormap,\n",
        "    extra_point_data=extra_data,\n",
        "    on_click=\"window.open(`{doi}`)\",custom_css=custom_css,\n",
        "    initial_zoom_fraction=0.7,enable_search=True, search_field='hover_text',\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqDMlG6-OBCP"
      },
      "outputs": [],
      "source": [
        "plot.save('science_base_map.html')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7hJKTp2UPKi9",
        "uMEsVYmrh2Lz",
        "uE1XekZKjaCE",
        "6EtARcFcingZ",
        "uM0gFcopRvcS",
        "CgXooRBGS3Cl",
        "zutNhfBOQfjX",
        "zwbsn9IUQfjZ",
        "P1pHIjTUQfja",
        "Z8AlH-s0OBB5",
        "z7BPauk9OBB_",
        "uRHf28NlOBCC",
        "lx86_0upOBCD",
        "qXqrOwk1OBCF",
        "GsYGVYk3OBCG",
        "F3dwTpptOBCI",
        "vJANy7XOOBCI",
        "hVI390zqOBCO"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
