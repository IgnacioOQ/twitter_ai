{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMzTc9H8OGHf"
      },
      "source": [
        "# EXPLANATION\n",
        "\n",
        "This is to get sample tweets for us to take a look and start labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_switch_setup"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# --- REPO ROOT ON sys.path (so `from src.*` works locally) ---\n",
        "_REPO_ROOT = str(Path(os.getcwd()).resolve().parents[1])\n",
        "if _REPO_ROOT not in sys.path:\n",
        "    sys.path.insert(0, _REPO_ROOT)\n",
        "\n",
        "\n",
        "# --- ENVIRONMENT SWITCH ---\n",
        "# Set to True if running on local machine with Google Drive Desktop mounted\n",
        "# Set to False if running in Google Colab cloud\n",
        "RUNNING_LOCALLY = True\n",
        "\n",
        "if RUNNING_LOCALLY:\n",
        "    # Standard macOS path for Google Drive Desktop\n",
        "    BASE_PATH = Path('/Volumes/GoogleDrive/MyDrive/AI Public Trust')\n",
        "else:\n",
        "    # Google Colab cloud path\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/AI Public Trust')\n",
        "\n",
        "# Pre-compute critical paths used across notebooks\n",
        "twits_folder = BASE_PATH / 'Raw Data/Twits/'\n",
        "test_folder = BASE_PATH / 'Raw Data/'\n",
        "datasets_folder = BASE_PATH / 'Data Sets'\n",
        "cleanedds_folder = BASE_PATH / 'Data Sets/Cleaned Data'\n",
        "networks_folder = BASE_PATH / 'Data Sets/Networks/'\n",
        "literature_folder = BASE_PATH / 'Literature/'\n",
        "topic_models_folder = BASE_PATH / 'Models/Topic Modeling/'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hJKTp2UPKi9"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 18722,
          "status": "ok",
          "timestamp": 1759248588209,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          },
          "user_tz": -120
        },
        "id": "A-R6lG_3OJ3G",
        "outputId": "621dbe65-cc4a-4db6-eace-b249893ad371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Current Directory: /content/drive/My Drive/Colab Projects/AI Public Trust/Raw Data/Twits\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import networkx as nx\n",
        "import re\n",
        "import string\n",
        "import csv\n",
        "\n",
        "from pathlib import Path\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1. Drive Mount & Paths\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Base project folder (Ignacio standard)\n",
        "BASE = Path('/content/drive/My Drive/Colab Projects/AI Public Trust')\n",
        "\n",
        "\n",
        "twits_folder = BASE / 'Raw Data/Twits/'\n",
        "test_folder = BASE / 'Raw Data/'\n",
        "print(\"Current Directory:\", twits_folder)\n",
        "datasets_folder = BASE / 'Data Sets'\n",
        "cleanedds_folder = BASE / 'Data Sets' / 'Cleaned Data'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanity Check"
      ],
      "metadata": {
        "id": "e0_7Fl1mUSXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the processing function that the authors of the classifier provide.\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)"
      ],
      "metadata": {
        "id": "96chR73FUTtW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759248588215,
          "user_tz": -120,
          "elapsed": 2,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how it looks, LINE BY LINE\n",
        "# CHECKING FOR TYPES OF TWEET THAT ARE NOT ORIGINAL\n",
        "AItrust_twits_dict_test = open(cleanedds_folder/'AItrust_pruned_twits_with_sentiment.json','r',encoding='utf-8')\n",
        "\n",
        "i = 0\n",
        "for line in AItrust_twits_dict_test:\n",
        "    twit = json.loads(line)\n",
        "    #print(twit)\n",
        "    try:\n",
        "      i+=1\n",
        "      print(twit.keys())\n",
        "      print(twit['sentiment_label'])\n",
        "      print(twit['public_metrics']['like_count'])\n",
        "      print(twit['public_metrics']['retweet_count'])\n",
        "      print(twit['type'])\n",
        "      print(twit.keys())\n",
        "      print('This is the original tweet:')\n",
        "      print(twit['text'])\n",
        "      print('\\n')\n",
        "      print('This is the clean tweet:')\n",
        "      print(preprocess(twit['text']))\n",
        "      print(twit['created_at'])\n",
        "      print('------')\n",
        "    except:\n",
        "      pass\n",
        "    #print('------')\n",
        "    if i>2:\n",
        "        break\n",
        "AItrust_twits_dict_test.close()"
      ],
      "metadata": {
        "id": "fb4LKYGJHgLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759248590301,
          "user_tz": -120,
          "elapsed": 2084,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "97d3ebde-b437-4d80-8b32-c5f6c057d053"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'conversation_id', 'processed_text', 'sentiment_label', 'sentiment_score'])\n",
            "positive\n",
            "1\n",
            "0\n",
            "original\n",
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'conversation_id', 'processed_text', 'sentiment_label', 'sentiment_score'])\n",
            "This is the original tweet:\n",
            "Hey \n",
            "you need perfect scores and completely AI proof content in your assignments and essaysü§©ü§©?\n",
            "We are your best made decision this semester we handle:\n",
            "‚úèMATLAB\n",
            "‚úèMathematics\n",
            "‚úèCoding\n",
            "‚úèAccounting/Finance\n",
            "‚úèWeb\n",
            "‚úèPython\n",
            "‚úèTableau, Excel, SPSS Statistics\n",
            "\n",
            "\n",
            "This is the clean tweet:\n",
            "Hey \n",
            "you need perfect scores and completely AI proof content in your assignments and essaysü§©ü§©?\n",
            "We are your best made decision this semester we handle:\n",
            "‚úèMATLAB\n",
            "‚úèMathematics\n",
            "‚úèCoding\n",
            "‚úèAccounting/Finance\n",
            "‚úèWeb\n",
            "‚úèPython\n",
            "‚úèTableau, Excel, SPSS Statistics\n",
            "2023-02-06T16:59:59.000Z\n",
            "------\n",
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'referenced_tweets', 'conversation_id', 'referenced_tweets_dictionary', 'processed_text', 'sentiment_label', 'sentiment_score'])\n",
            "neutral\n",
            "0\n",
            "368\n",
            "retweeted\n",
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'referenced_tweets', 'conversation_id', 'referenced_tweets_dictionary', 'processed_text', 'sentiment_label', 'sentiment_score'])\n",
            "This is the original tweet:\n",
            "RT @lallamapic: La Llama is checked ‚úÖü§£\n",
            "\n",
            "Who wants a draw for a Checks VV edition ($3200)‚ÅâÔ∏è\n",
            "\n",
            "Requirements:\n",
            "1. RT, Like &amp; Follow @lallamapic‚Ä¶\n",
            "\n",
            "\n",
            "This is the clean tweet:\n",
            "RT @user La Llama is checked ‚úÖü§£\n",
            "\n",
            "Who wants a draw for a Checks VV edition ($3200)‚ÅâÔ∏è\n",
            "\n",
            "Requirements:\n",
            "1. RT, Like &amp; Follow @user\n",
            "2023-02-06T16:59:59.000Z\n",
            "------\n",
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'referenced_tweets', 'conversation_id', 'referenced_tweets_dictionary', 'processed_text', 'sentiment_label', 'sentiment_score'])\n",
            "neutral\n",
            "0\n",
            "7565\n",
            "retweeted\n",
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'referenced_tweets', 'conversation_id', 'referenced_tweets_dictionary', 'processed_text', 'sentiment_label', 'sentiment_score'])\n",
            "This is the original tweet:\n",
            "RT @lexfridman: Humans are an API to ChatGPT.\n",
            "ChatGPT is an API to Python.\n",
            "Python is an API to C.\n",
            "C is an API to assembly.\n",
            "Assembly is an A‚Ä¶\n",
            "\n",
            "\n",
            "This is the clean tweet:\n",
            "RT @user Humans are an API to ChatGPT.\n",
            "ChatGPT is an API to Python.\n",
            "Python is an API to C.\n",
            "C is an API to assembly.\n",
            "Assembly is an A‚Ä¶\n",
            "2023-02-06T16:59:59.000Z\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Top Tweets"
      ],
      "metadata": {
        "id": "20snCpRZWC8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Define the number of top tweets you want\n",
        "TOP_N = 150\n",
        "# Total number of lines in the file for an accurate progress bar\n",
        "TOTAL_TWEETS = 17410035\n",
        "\n",
        "# Define your file paths using pathlib for better compatibility\n",
        "# Assumes 'cleanedds_folder' is a Path object or a string to your directory\n",
        "# If 'cleanedds_folder' isn't defined yet, uncomment and set the line below\n",
        "# cleanedds_folder = Path('path/to/your/data_folder')\n",
        "\n",
        "input_filepath = cleanedds_folder / 'AItrust_pruned_twits_with_sentiment.json'\n",
        "output_filepath = cleanedds_folder / f'top_{TOP_N}_tweets_by_likes_with_id.csv'\n",
        "\n",
        "# --- Main Logic ---\n",
        "all_tweets = []\n",
        "\n",
        "try:\n",
        "    with open(input_filepath, 'r', encoding='utf-8') as f:\n",
        "        print(f\"üöÄ Reading {TOTAL_TWEETS:,} tweets from {input_filepath}...\")\n",
        "        progress_bar = tqdm(f, total=TOTAL_TWEETS, desc=\"Processing tweets\", unit=\" tweets\")\n",
        "\n",
        "        for line in progress_bar:\n",
        "            try:\n",
        "                tweet = json.loads(line)\n",
        "                # Extract the necessary information, NOW INCLUDING THE TWEET ID\n",
        "                extracted_info = {\n",
        "                    'id': tweet.get('id'), # <-- ADDED THIS LINE\n",
        "                    'text': tweet.get('text'),\n",
        "                    'likes': tweet.get('public_metrics', {}).get('like_count', 0),\n",
        "                    'retweets': tweet.get('public_metrics', {}).get('retweet_count', 0),\n",
        "                    'sentiment': tweet.get('sentiment_label')\n",
        "                }\n",
        "                all_tweets.append(extracted_info)\n",
        "            except (json.JSONDecodeError, KeyError):\n",
        "                pass\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå ERROR: The input file was not found at {input_filepath}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\n‚úÖ Found and processed {len(all_tweets)} tweets.\")\n",
        "\n",
        "print(\"Sorting tweets by like count...\")\n",
        "sorted_tweets = sorted(all_tweets, key=lambda x: x['likes'], reverse=True)\n",
        "\n",
        "top_tweets = sorted_tweets[:TOP_N]\n",
        "\n",
        "print(f\"Writing the top {len(top_tweets)} tweets to {output_filepath} using pandas...\")\n",
        "\n",
        "# --- Write to CSV using pandas ---\n",
        "try:\n",
        "    # 1. Convert the list of dictionaries into a pandas DataFrame\n",
        "    df = pd.DataFrame(top_tweets)\n",
        "\n",
        "    # 2. (Optional but good practice) Define and set the column order\n",
        "    column_order = ['id', 'text', 'likes', 'retweets', 'sentiment']\n",
        "    df = df[column_order]\n",
        "\n",
        "    # 3. Use the DataFrame's .to_csv() method to save the file\n",
        "    df.to_csv(output_filepath, index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"üéâ Successfully created the CSV file!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Could not write to the file. Pandas error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS8m4ulBafFt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759249352249,
          "user_tz": -120,
          "elapsed": 300300,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "2a1c7b91-9081-49e9-aa00-8493fd612bbf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Reading 17,410,035 tweets from /content/drive/My Drive/Colab Projects/AI Public Trust/Data Sets/Cleaned Data/AItrust_pruned_twits_with_sentiment.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing tweets: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17410035/17410035 [04:53<00:00, 59347.84 tweets/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Found and processed 17410035 tweets.\n",
            "Sorting tweets by like count...\n",
            "Writing the top 150 tweets to /content/drive/My Drive/Colab Projects/AI Public Trust/Data Sets/Cleaned Data/top_150_tweets_by_likes_with_id.csv using pandas...\n",
            "üéâ Successfully created the CSV file!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "e0_7Fl1mUSXp"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
