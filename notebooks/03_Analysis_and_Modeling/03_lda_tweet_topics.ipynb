{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT5CpFLRWIq47QOxJqsYcL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_switch_setup"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# --- ENVIRONMENT SWITCH ---\n",
        "# Set to True if running on local machine with Google Drive Desktop mounted\n",
        "# Set to False if running in Google Colab cloud\n",
        "RUNNING_LOCALLY = False\n",
        "\n",
        "if RUNNING_LOCALLY:\n",
        "  # --- REPO ROOT ON sys.path (so `from src.*` works locally) ---\n",
        "    _REPO_ROOT = str(Path(os.getcwd()).resolve().parents[1])\n",
        "    if _REPO_ROOT not in sys.path:\n",
        "        sys.path.insert(0, _REPO_ROOT)\n",
        "    # Standard macOS path for Google Drive Desktop\n",
        "    BASE_PATH = Path('/Volumes/GoogleDrive/My Drive/Colab Projects/AI Public Trust')\n",
        "    \n",
        "else:\n",
        "    # Google Colab cloud path\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = Path('/content/drive/My Drive/Colab Projects/AI Public Trust')\n",
        "\n",
        "# Pre-compute critical paths used across notebooks\n",
        "twits_folder = BASE_PATH / 'Raw Data/Twits/'\n",
        "test_folder = BASE_PATH / 'Raw Data/'\n",
        "datasets_folder = BASE_PATH / 'Data Sets'\n",
        "cleanedds_folder = BASE_PATH / 'Data Sets/Cleaned Data'\n",
        "networks_folder = BASE_PATH / 'Data Sets/Networks/'\n",
        "literature_folder = BASE_PATH / 'Literature/'\n",
        "topic_models_folder = BASE_PATH / 'Models/Topic Modeling/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yi0_fTp92FP"
      },
      "outputs": [],
      "source": [
        "# # 3.3 Add LDA Topics to Tweets\n",
        "#\n",
        "# **Goal**: Train LDA on tweet text (using scikit-learn + CountVectorizer), evaluate (perplexity + coherence),\n",
        "# create diagnostics (pyLDAvis + t-SNE + UMAP), and **enrich the sentiment JSONL** with topic results (for K=5),\n",
        "# preserving all existing tweet fields. Artifacts are stored in Drive alongside prior steps.\n",
        "#\n",
        "# **Reads**:\n",
        "# - `/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/AItrust_pruned_twits_with_sentiment.json`\n",
        "#\n",
        "# **Writes (main)**:\n",
        "# - `/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/AItrust_pruned_twits_with_sentiment_and_topics_k5.json` (JSONL)\n",
        "# - Optional block files if chunking: `..._blockNNN.json`\n",
        "#\n",
        "# **Writes (models/diagnostics)** under `/content/drive/MyDrive/AI Public Trust/Models/Topic Modeling/LDA/`:\n",
        "# - `lda_k5_model.joblib`, `lda_k5_vectorizer.joblib`, `lda_grid_results.csv`\n",
        "# - `lda_k5_doc_topic_matrix.npy` (train set), `lda_k5_pyLDAvis.html`, `lda_k5_tsne.png`, `lda_k5_umap.png`\n",
        "# - `lda_k5_topics_metadata.json` (full) **and** a slim copy: `/.../Cleaned Data/AItrust_topics_k5_metadata.json`\n",
        "#\n",
        "# **Notes**:\n",
        "# - Keeps substantive elements of the rough script: grid over K, held-out perplexity, coherence (gensim c_v),\n",
        "#   pyLDAvis, t-SNE/UMAP, top terms, topic label dictionary; adds robust streaming enrichment to JSONL.\n",
        "# - Vectorizer/model are saved and reused to transform the full corpus in chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do this ONCE\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip -q install \"numpy==1.26.4\" \"scipy==1.10.1\"\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n",
        "import os; os.kill(os.getpid(), 9)  # force Colab runtime restart\n",
        "\n"
      ],
      "metadata": {
        "id": "dWWIJNpf_9ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff22196-4575-4afb-c756-fa834f0ee502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "cvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.3 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "jax 0.5.3 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "arviz 0.22.0 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip -q install \"gensim==4.3.2\" \"umap-learn==0.5.5\" \"pyLDAvis==3.4.1\"\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n"
      ],
      "metadata": {
        "id": "QEQ50a1z__0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 0. Environment & Imports\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# from google.colab import drive\n",
        "import os, re, json, glob, math, gc, sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Modeling stack\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# UMAP for visualization\n",
        "try:\n",
        "    import umap\n",
        "except ImportError:\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip -q install umap-learn==0.5.5\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n",
        "    import umap\n",
        "\n",
        "# Gensim for coherence\n",
        "try:\n",
        "    from gensim.corpora import Dictionary as GensimDictionary\n",
        "    from gensim.models import CoherenceModel\n",
        "except Exception:\n",
        "import os\n",
        "if not RUNNING_LOCALLY:\n",
        "    print('Running Colab setup shell commands...')\n",
        "    !pip -q install gensim==4.3.2\n",
        "else:\n",
        "    print('Running locally: Skipping Colab shell setup.')\n",
        "\n",
        "    from gensim.corpora import Dictionary as GensimDictionary\n",
        "    from gensim.models import CoherenceModel\n",
        "\n",
        "# NLTK for tokenization/stopwords (for coherence pipeline)\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# pyLDAvis\n",
        "import importlib, subprocess, sys\n",
        "try:\n",
        "    import pyLDAvis\n",
        "    try:\n",
        "        from pyLDAvis.sklearn import prepare as sklearn_lda_prepare\n",
        "    except ImportError:\n",
        "        from pyLDAvis.sklearn_model import prepare as sklearn_lda_prepare\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyLDAvis==3.2.2\"])\n",
        "    import pyLDAvis\n",
        "    from pyLDAvis.sklearn import prepare as sklearn_lda_prepare\n",
        "\n",
        "# Persistence\n",
        "from joblib import dump, load\n"
      ],
      "metadata": {
        "id": "G_vb9NLs-AV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1. Drive Mount & Paths\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Base project folder (Ignacio standard)\n",
        "# BASE = Path('/content/drive/My Drive/Colab Projects/AI Public Trust')\n",
        "DATA_DIR = BASE / 'Data Sets' / 'Cleaned Data'\n",
        "MODELS_DIR = BASE / 'Models' / 'Topic Modeling' / 'LDA'\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Inputs\n",
        "INPUT_JSONL = DATA_DIR / 'AItrust_pruned_twits_with_sentiment.json'\n",
        "ORIG_INPUT_JSONL = DATA_DIR / 'AItrust_pruned_twits_with_sentiment.json'\n",
        "\n",
        "# Main enrichment output\n",
        "OUTPUT_JSONL = DATA_DIR / 'AItrust_pruned_twits_with_sentiment_and_topics_k5.json'\n",
        "# Slim topics metadata copy lives in Cleaned Data\n",
        "SLIM_META_JSON = DATA_DIR / 'AItrust_topics_k5_metadata.json'\n",
        "\n",
        "# Model artifacts\n",
        "LDA_MODEL_PATH = MODELS_DIR / 'lda_k5_model.joblib'\n",
        "VECT_PATH      = MODELS_DIR / 'lda_k5_vectorizer.joblib'\n",
        "DOC_TOPIC_NPY  = MODELS_DIR / 'lda_k5_doc_topic_matrix.npy'\n",
        "PYLDAVIS_HTML  = MODELS_DIR / 'lda_k5_pyLDAvis.html'\n",
        "TSNE_PNG       = MODELS_DIR / 'lda_k5_tsne.png'\n",
        "UMAP_PNG       = MODELS_DIR / 'lda_k5_umap.png'\n",
        "GRID_CSV       = MODELS_DIR / 'lda_grid_results.csv'\n",
        "FULL_META_JSON = MODELS_DIR / 'lda_k5_topics_metadata.json'\n",
        "\n",
        "# Optional block writing (for very large corpora)\n",
        "BLOCK_BASENAME = OUTPUT_JSONL.stem + '_block'\n",
        "\n",
        "print('INPUT  :', INPUT_JSONL)\n",
        "print('OUTPUT :', OUTPUT_JSONL)\n",
        "print('MODELS :', MODELS_DIR)\n"
      ],
      "metadata": {
        "id": "yPKPf8Rv-LPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 2. Parameters (kept explicit)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Training/sample controls\n",
        "K_TARGET = 5                       # K for enrichment file\n",
        "#TRAIN_SAMPLE_MAX = 5000           # cap number of docs for model training (adjust if needed)\n",
        "TRAIN_SAMPLE_MAX = 25000           # cap number of docs for model training (adjust if needed)\n",
        "TEST_SIZE = 0.10                   # held-out fraction for perplexity\n",
        "RANDOM_STATE = 0\n",
        "SEEDS = [0]                        # can extend e.g., [0,1,2]\n",
        "\n",
        "# Grid for diagnostics (perplexity + coherence)\n",
        "TOPIC_GRID = [3, 5, 8, 10, 12, 15, 20, 25, 30]\n",
        "\n",
        "# Vectorizer parameters (kept close to rough script)\n",
        "VECT_KW = dict(\n",
        "    max_df=0.95,\n",
        "    min_df=5,\n",
        ")\n",
        "\n",
        "# LDA parameters (scikit-learn)\n",
        "LDA_KW = dict(\n",
        "    learning_method='batch',   # consistent, deterministic per seed\n",
        "    learning_decay=0.7,        # matches rough script default\n",
        "    max_iter=20,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "# Chunking for full-corpus transform\n",
        "READ_CHUNK = 100_000            # number of lines per streaming chunk\n",
        "WRITE_BLOCKS = True             # write block files to avoid a single massive write\n",
        "\n",
        "# Visualization parameters\n",
        "TSNE_KW = dict(n_components=2, learning_rate='auto', init='random', perplexity=30, random_state=RANDOM_STATE)\n",
        "UMAP_KW = dict(n_neighbors=25, min_dist=0.10, n_components=2, metric='cosine', random_state=RANDOM_STATE)\n",
        "\n"
      ],
      "metadata": {
        "id": "HpCnmsh6-S6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 3. Utilities: tweet text normalization & tokenization for coherence\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Light, non-destructive tweet cleanup (kept parameterized)\n",
        "URL_RE = re.compile(r'https?://\\S+')\n",
        "MENTION_RE = re.compile(r'@\\w+')\n",
        "HASHTAG_RE = re.compile(r'#(\\w+)')\n",
        "WHITESPACE_RE = re.compile(r'\\s+')\n",
        "\n",
        "EN_STOP = set(stopwords.words('english'))\n",
        "TOKENIZER = RegexpTokenizer(r\"[A-Za-z][A-Za-z_\\-']+\")\n",
        "\n",
        "\n",
        "def normalize_text(t: str) -> str:\n",
        "    if not isinstance(t, str):\n",
        "        return ''\n",
        "    t = t.replace('RT ', ' ')                       # drop RT marker\n",
        "    t = URL_RE.sub(' ', t)\n",
        "    t = MENTION_RE.sub(' ', t)\n",
        "    # keep hashtag terms without '#'\n",
        "    t = HASHTAG_RE.sub(lambda m: ' ' + m.group(1) + ' ', t)\n",
        "    t = t.lower()\n",
        "    t = WHITESPACE_RE.sub(' ', t).strip()\n",
        "    return t\n",
        "\n",
        "\n",
        "def simple_tokenize(t: str):\n",
        "    t = normalize_text(t)\n",
        "    toks = TOKENIZER.tokenize(t)\n",
        "    return [w for w in toks if w not in EN_STOP and len(w) > 1]\n"
      ],
      "metadata": {
        "id": "U8scBDb2-TZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 4. Read a training sample from JSONL (stream-safe)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "def stream_jsonl(path: Path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            try:\n",
        "                yield json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "# Collect up to TRAIN_SAMPLE_MAX docs for training/validation\n",
        "train_docs = []\n",
        "train_ids  = []\n",
        "for i, rec in enumerate(stream_jsonl(INPUT_JSONL)):\n",
        "    text = rec.get('text', '')\n",
        "    if not text:\n",
        "        continue\n",
        "    train_docs.append(normalize_text(text))\n",
        "    train_ids.append(rec.get('id'))\n",
        "    if len(train_docs) >= TRAIN_SAMPLE_MAX:\n",
        "        break\n",
        "\n",
        "print(f\"Loaded {len(train_docs):,} training docs from JSONL (cap={TRAIN_SAMPLE_MAX:,}).\")\n"
      ],
      "metadata": {
        "id": "FFGDtgkv-VVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 5. Vectorize & split\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "X_train_full = None\n",
        "vect = CountVectorizer(**VECT_KW)\n",
        "X_all = vect.fit_transform(train_docs)\n",
        "y_dummy = np.zeros(len(train_docs))  # placeholder labels (not used)\n",
        "\n",
        "X_train, X_test, _, _ = train_test_split(X_all, y_dummy, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "print(\"Vectorized. Shapes:\", X_train.shape, X_test.shape)\n"
      ],
      "metadata": {
        "id": "UKIclFze-W6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 6. Grid search over K: train, save, evaluate (perplexity) & collect results\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Initialize/append CSV\n",
        "if GRID_CSV.exists():\n",
        "    res_df = pd.read_csv(GRID_CSV)\n",
        "else:\n",
        "    res_df = pd.DataFrame(columns=[\"k\", \"seed\", \"perplexity\"])\n",
        "\n",
        "for k in TOPIC_GRID:\n",
        "    for seed in SEEDS:\n",
        "        tag = f\"test_k{k}_s{seed}\"\n",
        "        model_path = MODELS_DIR / f\"{tag}.joblib\"\n",
        "        docTopic_path = MODELS_DIR / f\"{tag}_docTopic.joblib\"\n",
        "\n",
        "        if model_path.exists() and docTopic_path.exists():\n",
        "            print(f\"‚è©  {tag} exists ‚Äì skipping train.\")\n",
        "            continue\n",
        "\n",
        "        lda = LatentDirichletAllocation(n_components=k, random_state=seed, **{k2:v for k2,v in LDA_KW.items() if k2!='random_state'})\n",
        "        lda.fit(X_train)\n",
        "        doc_topic = lda.transform(X_train)  # train doc-topic\n",
        "\n",
        "        # Perplexity on test set\n",
        "        perp = lda.perplexity(X_test)\n",
        "        print(f\"‚úÖ k={k}, seed={seed} ‚Üí Perplexity(Test) = {perp:.2f}\")\n",
        "\n",
        "        # Save artifacts for this grid point\n",
        "        dump(lda, model_path)\n",
        "        dump(doc_topic, docTopic_path)\n",
        "\n",
        "        # Append results row\n",
        "        res_df.loc[len(res_df)] = {\"k\":k, \"seed\":seed, \"perplexity\":perp}\n",
        "        res_df.to_csv(GRID_CSV, index=False)\n",
        "\n",
        "# Plot Perplexity vs K (avg over seeds)\n",
        "if len(res_df):\n",
        "    avg_perp = res_df.groupby('k')['perplexity'].mean().sort_index()\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(avg_perp.index, avg_perp.values, marker='o')\n",
        "    plt.title('Held-out Perplexity vs Number of Topics (k)')\n",
        "    plt.xlabel('Number of Topics (k)')\n",
        "    plt.ylabel('Avg Perplexity (Test)')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dokWu2my-Ymb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 7. Coherence (gensim c_v) using simple_tokenize ‚Üí Dictionary ‚Üí CoherenceModel\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Tokenize the same training docs (note: different pipeline than CountVectorizer)\n",
        "print(\"Tokenizing training docs for coherence (gensim c_v)‚Ä¶\")\n",
        "tokenized_docs = [simple_tokenize(t) for t in train_docs]\n",
        "gensim_dict = GensimDictionary(tokenized_docs)\n",
        "print(f\"‚úÖ Tokenized {len(tokenized_docs):,} documents; vocab size={len(gensim_dict):,}\")\n",
        "\n",
        "# Helper: extract top-N words per topic from a fitted sklearn LDA\n",
        "\n",
        "def _top_words_from_sklearn_lda(lda_model, vocab, topn=15):\n",
        "    comp = lda_model.components_  # shape: (k, vocab_size)\n",
        "    top = {}\n",
        "    for tid, row in enumerate(comp):\n",
        "        top_idx = np.argsort(row)[::-1][:topn]\n",
        "        top[tid] = [(vocab[i], float(row[i])) for i in top_idx]\n",
        "    return top\n",
        "\n",
        "# Compute coherence for each (k, seed)\n",
        "coherence_scores = {}\n",
        "for k in TOPIC_GRID:\n",
        "    for seed in SEEDS:\n",
        "        tag = f\"test_k{k}_s{seed}\"\n",
        "        model_path = MODELS_DIR / f\"{tag}.joblib\"\n",
        "        if not model_path.exists():\n",
        "            continue\n",
        "        lda_model = load(model_path)\n",
        "        vocab = np.array(vect.get_feature_names_out())\n",
        "        top_terms = _top_words_from_sklearn_lda(lda_model, vocab, topn=15)\n",
        "        # Build gensim CoherenceModel using tokenized_docs & top terms per topic\n",
        "        cm = CoherenceModel(\n",
        "            topics=[[w for (w,_) in top_terms[t]] for t in sorted(top_terms)],\n",
        "            texts=tokenized_docs,\n",
        "            dictionary=gensim_dict,\n",
        "            coherence='c_v',\n",
        "        )\n",
        "        cv = float(cm.get_coherence())\n",
        "        coherence_scores[(k, seed)] = cv\n",
        "        print(f\"‚úÖ k={k}, seed={seed} ‚Üí c_v = {cv:.3f}\")\n",
        "\n",
        "# Plot Coherence vs K (avg over seeds)\n",
        "if len(coherence_scores):\n",
        "    from collections import defaultdict\n",
        "    buckets = defaultdict(list)\n",
        "    for (k, seed), score in coherence_scores.items():\n",
        "        buckets[k].append(score)\n",
        "    avg_cv = {k: sum(v)/len(v) for k, v in buckets.items()}\n",
        "    sk = sorted(avg_cv)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(sk, [avg_cv[k] for k in sk], marker='o')\n",
        "    plt.title('Topic Coherence (c_v) vs Number of Topics (k)')\n",
        "    plt.xlabel('Number of Topics (k)')\n",
        "    plt.ylabel('Avg Coherence (c_v)')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Ay401uiY-as9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 8. Train final LDA at K_TARGET and create diagnostics (pyLDAvis, t-SNE, UMAP)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(f\"\\nTraining final LDA with K={K_TARGET} for enrichment‚Ä¶\")\n",
        "lda_k = LatentDirichletAllocation(n_components=K_TARGET, random_state=RANDOM_STATE, **{k2:v for k2,v in LDA_KW.items() if k2!='random_state'})\n",
        "lda_k.fit(X_train)\n",
        "train_doc_topic = lda_k.transform(X_train)\n",
        "\n",
        "# Save model/vectorizer + train doc-topic\n",
        "dump(lda_k, LDA_MODEL_PATH)\n",
        "dump(vect, VECT_PATH)\n",
        "np.save(DOC_TOPIC_NPY, train_doc_topic)\n",
        "print(\"Saved:\", LDA_MODEL_PATH.name, VECT_PATH.name, DOC_TOPIC_NPY.name)\n",
        "\n",
        "# pyLDAvis panel (on training set)\n",
        "print(\"Preparing pyLDAvis panel‚Ä¶\")\n",
        "class _VectCompat:\n",
        "    def __init__(self, v):\n",
        "        self.v = v\n",
        "    def get_feature_names(self):\n",
        "        # pyLDAvis expects a list-like; convert to list for safety\n",
        "        return list(self.v.get_feature_names_out())\n",
        "\n",
        "vect_for_vis = _VectCompat(vect)\n",
        "vis_panel = sklearn_lda_prepare(lda_k, X_train, vect_for_vis, mds='pcoa')\n",
        "pyLDAvis.save_html(vis_panel, str(PYLDAVIS_HTML))\n",
        "print(\"Saved:\", PYLDAVIS_HTML.name)\n",
        "\n",
        "# Visualize train doc-topic with TSNE & UMAP (colored by argmax topic)\n",
        "dom_topic = train_doc_topic.argmax(axis=1)\n",
        "\n",
        "print(\"t-SNE plot‚Ä¶\")\n",
        "tsne = TSNE(**TSNE_KW)\n",
        "tsne_xy = tsne.fit_transform(train_doc_topic)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(tsne_xy[:,0], tsne_xy[:,1], c=dom_topic, s=5)\n",
        "plt.title(f\"t-SNE of doc‚Äìtopic vectors (k={K_TARGET})\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(TSNE_PNG, dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved:\", TSNE_PNG.name)\n",
        "\n",
        "print(\"UMAP plot‚Ä¶\")\n",
        "reducer = umap.UMAP(**UMAP_KW)\n",
        "umap_xy = reducer.fit_transform(train_doc_topic)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(umap_xy[:,0], umap_xy[:,1], c=dom_topic, s=5)\n",
        "plt.title(f\"UMAP of doc‚Äìtopic vectors (k={K_TARGET})\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(UMAP_PNG, dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved:\", UMAP_PNG.name)\n"
      ],
      "metadata": {
        "id": "FaV88uv4-czV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 9. Topic labels & metadata exports\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Optional human labels (extend or customize to your taxonomy)\n",
        "# Keep structure compatible with rough script: mapping by K\n",
        "TOPIC_LABELS = {\n",
        "    5: {\n",
        "        0: \"Topic 0\",\n",
        "        1: \"Topic 1\",\n",
        "        2: \"Topic 2\",\n",
        "        3: \"Topic 3\",\n",
        "        4: \"Topic 4\",\n",
        "    },\n",
        "    # You can add alternative K labelings here (8, 12, ‚Ä¶) if desired\n",
        "}\n",
        "\n",
        "# Build metadata for K_TARGET\n",
        "vocab = np.array(vect.get_feature_names_out())\n",
        "components = lda_k.components_\n",
        "TOPN = 15\n",
        "meta_topics = []\n",
        "for tid in range(components.shape[0]):\n",
        "    idx = np.argsort(components[tid])[::-1][:TOPN]\n",
        "    terms = [\n",
        "        {\"term\": vocab[i], \"weight\": float(components[tid, i])}\n",
        "        for i in idx\n",
        "    ]\n",
        "    meta_topics.append({\n",
        "        \"topic_id\": tid,\n",
        "        \"label\": TOPIC_LABELS.get(K_TARGET, {}).get(tid, f\"Topic {tid}\"),\n",
        "        \"top_terms\": terms,\n",
        "    })\n",
        "\n",
        "full_meta = {\n",
        "    \"k\": K_TARGET,\n",
        "    \"topics\": meta_topics,\n",
        "    \"vocabulary_size\": int(len(vocab)),\n",
        "    \"vectorizer_params\": VECT_KW,\n",
        "    \"lda_params\": {**LDA_KW, \"n_components\": K_TARGET},\n",
        "}\n",
        "\n",
        "with open(FULL_META_JSON, 'w', encoding='utf-8') as f:\n",
        "    json.dump(full_meta, f, ensure_ascii=False, indent=2)\n",
        "print(\"Saved:\", FULL_META_JSON.name)\n",
        "\n",
        "# Slim quick-reference copy in Cleaned Data\n",
        "slim_topics = {\n",
        "    \"k\": K_TARGET,\n",
        "    \"topics\": [\n",
        "        {\n",
        "            \"topic_id\": t[\"topic_id\"],\n",
        "            \"label\": t[\"label\"],\n",
        "            \"top_terms\": [w[\"term\"] for w in t[\"top_terms\"]],\n",
        "        }\n",
        "        for t in meta_topics\n",
        "    ],\n",
        "}\n",
        "with open(SLIM_META_JSON, 'w', encoding='utf-8') as f:\n",
        "    json.dump(slim_topics, f, ensure_ascii=False, indent=2)\n",
        "print(\"Saved:\", SLIM_META_JSON.name)\n"
      ],
      "metadata": {
        "id": "hB1p7C5e-fzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Toggle: limit enrichment to a small sample without editing the main loop ---\n",
        "SAMPLE_ENRICH   = False        # False = full run\n",
        "SAMPLE_ENRICH_N = 50_000      # how many lines for the sample\n",
        "\n",
        "# Always sample from the original full file\n",
        "SOURCE = ORIG_INPUT_JSONL\n",
        "\n",
        "if SAMPLE_ENRICH:\n",
        "    sample_path = DATA_DIR / f\"{SOURCE.stem}__SAMPLE{SAMPLE_ENRICH_N}.json\"\n",
        "    n = 0\n",
        "    with open(SOURCE, 'r', encoding='utf-8') as fin, open(sample_path, 'w', encoding='utf-8') as fout:\n",
        "        for line in fin:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            fout.write(line)\n",
        "            n += 1\n",
        "            if n >= SAMPLE_ENRICH_N:\n",
        "                break\n",
        "    INPUT_JSONL    = sample_path\n",
        "    OUTPUT_JSONL   = DATA_DIR / f\"AItrust_pruned_twits_with_sentiment_and_topics_k5__SAMPLE{SAMPLE_ENRICH_N}.json\"\n",
        "    BLOCK_BASENAME = OUTPUT_JSONL.stem + \"_block\"\n",
        "    print(f\"üîé Enrichment LIMITED to first {n:,} lines ‚Üí using: {sample_path.name}\")\n",
        "    print(f\"üìù Output will be: {OUTPUT_JSONL.name}\")\n",
        "else:\n",
        "    INPUT_JSONL    = ORIG_INPUT_JSONL\n",
        "    OUTPUT_JSONL   = DATA_DIR / \"AItrust_pruned_twits_with_sentiment_and_topics_k5.json\"\n",
        "    BLOCK_BASENAME = OUTPUT_JSONL.stem + \"_block\"\n",
        "    print(f\"üöÄ Enrichment will run on FULL file: {INPUT_JSONL.name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pfgMoFdSKTFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 10. Stream the full sentiment JSONL ‚Üí add topics (K=5) ‚Üí write JSONL (+ blocks)\n",
        "# (BATCHED version: faster, same outputs)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from itertools import islice\n",
        "import time, gc\n",
        "\n",
        "# Load persisted vectorizer & model to ensure consistency\n",
        "vect = load(VECT_PATH)\n",
        "lda_k = load(LDA_MODEL_PATH)\n",
        "\n",
        "# Ensure output parent exists\n",
        "OUTPUT_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Clear only this step's old block files\n",
        "if WRITE_BLOCKS:\n",
        "    for old in DATA_DIR.glob(BLOCK_BASENAME + \"*.json\"):\n",
        "        try:\n",
        "            old.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# ---- speed knobs (tune as RAM allows) ----\n",
        "BATCH_DOCS       = 50_000    # try 50k; drop to 20k if RAM tight\n",
        "WRITE_BLOCK_SIZE = 200_000   # write to disk every ~200k lines\n",
        "PROGRESS_EVERY   = 50_000\n",
        "\n",
        "# ---- state ----\n",
        "block_idx = 1\n",
        "written_total = 0\n",
        "processed_total = 0\n",
        "write_buffer = []\n",
        "start_time = time.time()\n",
        "\n",
        "def flush_block():\n",
        "    \"\"\"Write accumulated lines to a block file.\"\"\"\n",
        "    global write_buffer, written_total, block_idx\n",
        "    if not write_buffer:\n",
        "        return\n",
        "    if WRITE_BLOCKS:\n",
        "        block_path = DATA_DIR / f\"{BLOCK_BASENAME}{block_idx:03d}.json\"\n",
        "        with open(block_path, 'w', encoding='utf-8') as fout:\n",
        "            fout.write(\"\\n\".join(write_buffer) + \"\\n\")\n",
        "        print(f\"Wrote block {block_idx:03d} with {len(write_buffer):,} lines ‚Üí {block_path.name}\")\n",
        "        block_idx += 1\n",
        "    written_total += len(write_buffer)\n",
        "    write_buffer = []\n",
        "\n",
        "def process_batch(records):\n",
        "    \"\"\"Batch-transform texts; add LDA fields; append JSON lines to buffer.\"\"\"\n",
        "    global processed_total\n",
        "\n",
        "    # collect texts to transform\n",
        "    idxs, texts = [], []\n",
        "    for i, r in enumerate(records):\n",
        "        t = r.get(\"text\", \"\")\n",
        "        if isinstance(t, str) and t:\n",
        "            idxs.append(i)\n",
        "            texts.append(normalize_text(t))\n",
        "\n",
        "    # one vectorize+transform per batch\n",
        "    if idxs:\n",
        "        Xb = vect.transform(texts)\n",
        "        Db = lda_k.transform(Xb)              # (len(idxs), K)\n",
        "        argmax = Db.argmax(axis=1)\n",
        "        for j, i_rec in enumerate(idxs):\n",
        "            dist = Db[j].tolist()\n",
        "            tid  = int(argmax[j])\n",
        "            label = TOPIC_LABELS.get(K_TARGET, {}).get(tid, f\"Topic {tid}\")\n",
        "            r = records[i_rec]\n",
        "            r[\"lda_k5_topic_id\"]   = tid\n",
        "            r[\"lda_k5_topic_dist\"] = [float(x) for x in dist]\n",
        "            r[\"lda_k5_topic_label\"]= label\n",
        "\n",
        "    # serialize all records (including ones without text)\n",
        "    for r in records:\n",
        "        write_buffer.append(json.dumps(r, ensure_ascii=False))\n",
        "\n",
        "    processed_total += len(records)\n",
        "    if processed_total % PROGRESS_EVERY == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = processed_total / max(elapsed, 1e-9)\n",
        "        print(f\"Progress: {processed_total:,} processed | {rate:,.0f}/s | {int(elapsed)}s elapsed\")\n",
        "\n",
        "    if len(write_buffer) >= WRITE_BLOCK_SIZE:\n",
        "        flush_block()\n",
        "\n",
        "with open(INPUT_JSONL, 'r', encoding='utf-8') as fin:\n",
        "    while True:\n",
        "        lines = list(islice(fin, BATCH_DOCS))\n",
        "        if not lines:\n",
        "            break\n",
        "\n",
        "        # parse JSON lines\n",
        "        batch_records = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                batch_records.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "        process_batch(batch_records)\n",
        "        gc.collect()\n",
        "\n",
        "# final flush + merge\n",
        "flush_block()\n",
        "\n",
        "if WRITE_BLOCKS:\n",
        "    print(\"Merging blocks ‚Üí\", OUTPUT_JSONL.name)\n",
        "    with open(OUTPUT_JSONL, 'w', encoding='utf-8') as fout:\n",
        "        for blk in sorted(DATA_DIR.glob(BLOCK_BASENAME + \"*.json\")):\n",
        "            with open(blk, 'r', encoding='utf-8') as fin:\n",
        "                for line in fin:\n",
        "                    fout.write(line)\n",
        "    print(\"‚úÖ Merged\", len(list(DATA_DIR.glob(BLOCK_BASENAME + \"*.json\"))), \"blocks ‚Üí\", OUTPUT_JSONL.name)\n",
        "\n",
        "elapsed_total = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Enrichment done. Lines written: {written_total:,} | \"\n",
        "      f\"Processed: {processed_total:,} | Elapsed: {int(elapsed_total)}s\")\n"
      ],
      "metadata": {
        "id": "X6rCRyf7-h1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 11. (Optional) Merge block files ‚Üí single JSONL\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if WRITE_BLOCKS:\n",
        "    print(\"Merging blocks ‚Üí\", OUTPUT_JSONL.name)\n",
        "    with open(OUTPUT_JSONL, 'w', encoding='utf-8') as fout:\n",
        "        for blk in sorted(DATA_DIR.glob(BLOCK_BASENAME + \"*.json\")):\n",
        "            with open(blk, 'r', encoding='utf-8') as fin:\n",
        "                for line in fin:\n",
        "                    fout.write(line)\n",
        "    print(\"‚úÖ Merged\", len(list(DATA_DIR.glob(BLOCK_BASENAME + \"*.json\"))), \"blocks ‚Üí\", OUTPUT_JSONL.name)\n"
      ],
      "metadata": {
        "id": "QXftaqx5-kKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 12. Sanity check: peek a few records\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(\"\\nSample enriched records:\")\n",
        "peek_n = 3\n",
        "with open(OUTPUT_JSONL, 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= peek_n:\n",
        "            break\n",
        "        try:\n",
        "            rec = json.loads(line)\n",
        "            print({\n",
        "                'id': rec.get('id'),\n",
        "                'topic_id': rec.get('lda_k5_topic_id'),\n",
        "                'topic_label': rec.get('lda_k5_topic_label'),\n",
        "                'topic_dist_0_2': [round(x, 3) for x in rec.get('lda_k5_topic_dist', [])[:3]],\n",
        "                'sentiment': rec.get('sentiment'),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print('Decode error on peek line', i, e)\n"
      ],
      "metadata": {
        "id": "nvKlMhO3-nMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvnURRNkPxbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bm1hd4u6JKEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mebW4gZWJKGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mdWP8bFJKJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fc2KcYhWJKLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12yUSBU6PxfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GYOefF9P_9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVn8k9caKdUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nt8q7ogeKdW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dv-SG2qQKdZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ecd49E7IKdb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5h_W_jllKdef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wAjN-Z0KdhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDmNWO1rKdjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jekF95-HKdmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggxOe9gJKdpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTIxX68QKdrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "72undLkRKduN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0G7GSZSNKdwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a7XcCopIKdzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmlSbU9MKd2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Temporary* -- the merge tripped because Google Drive‚Äôs FUSE mount glitched (‚ÄúTransport endpoint is not connected‚Äù)."
      ],
      "metadata": {
        "id": "bS-MVvJlKeRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l \"/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/AItrust_pruned_twits_with_sentiment_and_topics_k5.json\"\n"
      ],
      "metadata": {
        "id": "LvkMf_eoyCQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b09ld5NV7m23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aANEtC9p7m9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, gzip, os, io\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths (match your project layout)\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data\")\n",
        "BASENAME = \"AItrust_pruned_twits_with_sentiment_and_topics_k5\"\n",
        "OUT_GZ   = DATA_DIR / f\"{BASENAME}.jsonl.gz\"\n",
        "OUT_PART = DATA_DIR / f\"{BASENAME}.jsonl.gz.part\"   # temp file, renamed on success\n",
        "\n",
        "# 1) Gather block files in numeric order\n",
        "pat = re.compile(rf\"^{re.escape(BASENAME)}_block(\\d+)\\.json$\")\n",
        "blocks = []\n",
        "for p in DATA_DIR.glob(f\"{BASENAME}_block*.json\"):\n",
        "    m = pat.match(p.name)\n",
        "    if m:\n",
        "        blocks.append((int(m.group(1)), p))\n",
        "blocks.sort(key=lambda x: x[0])\n",
        "block_paths = [p for _, p in blocks]\n",
        "\n",
        "if not block_paths:\n",
        "    raise SystemExit(\"No block files found. Nothing to merge.\")\n",
        "\n",
        "print(f\"Found {len(block_paths)} block(s). First: {block_paths[0].name}  Last: {block_paths[-1].name}\")\n",
        "\n",
        "# 2) Stream-merge ‚Üí gzip (write to .part first)\n",
        "lines = 0\n",
        "FLUSH_EVERY = 500_000\n",
        "\n",
        "# If a previous .part exists (e.g., after an interruption), remove it\n",
        "try:\n",
        "    OUT_PART.unlink()\n",
        "except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "with gzip.open(OUT_PART, \"wt\", encoding=\"utf-8\", compresslevel=6) as fout:\n",
        "    for i, blk in enumerate(block_paths, 1):\n",
        "        with blk.open(\"r\", encoding=\"utf-8\") as fin:\n",
        "            for line in fin:\n",
        "                fout.write(line)\n",
        "                lines += 1\n",
        "                if lines % FLUSH_EVERY == 0:\n",
        "                    # progress indicator\n",
        "                    print(f\"Progress: {lines:,} lines written (through block {i}/{len(block_paths)})\")\n",
        "\n",
        "print(f\"Merge-to-gzip complete ‚Üí {OUT_PART.name}  |  Lines: {lines:,}\")\n",
        "\n",
        "# 3) Quick gzip integrity check\n",
        "rc = os.system(f'gzip -t \"{OUT_PART}\"')\n",
        "if rc != 0:\n",
        "    raise SystemExit(\"Gzip integrity check failed. Do not rename .part; please rerun this cell.\")\n",
        "\n",
        "# 4) Rename .part ‚Üí final (best-effort atomic on Drive)\n",
        "try:\n",
        "    OUT_GZ.unlink()  # remove old final if present\n",
        "except FileNotFoundError:\n",
        "    pass\n",
        "OUT_PART.rename(OUT_GZ)\n",
        "print(f\"‚úÖ Final file ready: {OUT_GZ.name}  |  Total lines (written): {lines:,}\")\n"
      ],
      "metadata": {
        "id": "R63AoHIf7nAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zcat \"/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/AItrust_pruned_twits_with_sentiment_and_topics_k5.jsonl.gz\" | wc -l\n"
      ],
      "metadata": {
        "id": "xJ4JQuRz7sRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip, json, itertools, random\n",
        "p = \"/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/AItrust_pruned_twits_with_sentiment_and_topics_k5.jsonl.gz\"\n",
        "with gzip.open(p, \"rt\", encoding=\"utf-8\") as f:\n",
        "    sample = [json.loads(x) for x in itertools.islice(f, 10)]\n",
        "sample[:3]\n"
      ],
      "metadata": {
        "id": "gy5rSbtpK0bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7XqZIl_ah-aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Analysis stuff (to redo properly)"
      ],
      "metadata": {
        "id": "BPxxdQchh-v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip, json\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/AItrust_pruned_twits_with_sentiment_and_topics_k5.jsonl.gz\"\n",
        "\n",
        "# --- Pass 1: collect counts + text samples ---\n",
        "topic_counts = Counter()\n",
        "sentiment_counts = Counter()\n",
        "topic_sentiment_counts = defaultdict(Counter)\n",
        "topic_texts = defaultdict(list)  # hold a small sample of texts per topic\n",
        "topic_corpus = defaultdict(list) # collect texts for n-gram extraction\n",
        "\n",
        "MAX_SAMPLE_PER_TOPIC = 20_000   # cap so we don't overload RAM\n",
        "\n",
        "with gzip.open(DATA_PATH, \"rt\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f, 1):\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        t_id = obj.get(\"lda_k5_topic_id\")\n",
        "        sent = obj.get(\"sentiment_label\")\n",
        "        txt = obj.get(\"text\", \"\")\n",
        "\n",
        "        if t_id is None or not txt:\n",
        "            continue\n",
        "\n",
        "        topic_counts[t_id] += 1\n",
        "        sentiment_counts[sent] += 1\n",
        "        topic_sentiment_counts[t_id][sent] += 1\n",
        "\n",
        "        if len(topic_texts[t_id]) < 3:\n",
        "            topic_texts[t_id].append(txt)\n",
        "\n",
        "        if len(topic_corpus[t_id]) < MAX_SAMPLE_PER_TOPIC:\n",
        "            topic_corpus[t_id].append(txt)\n",
        "\n",
        "print(\"Pass 1 complete\")\n",
        "\n",
        "# --- Pass 2: extract top n-grams per topic ---\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1,2), max_features=5000)\n",
        "\n",
        "topic_top_terms = {}\n",
        "for t_id, texts in topic_corpus.items():\n",
        "    if not texts:\n",
        "        continue\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    freqs = X.sum(axis=0).A1\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    top_idx = freqs.argsort()[::-1][:15]\n",
        "    topic_top_terms[t_id] = [(terms[i], freqs[i]) for i in top_idx]\n",
        "\n",
        "print(\"Pass 2 complete\")\n",
        "\n",
        "# --- Display nicely ---\n",
        "print(\"\\n=== Global Sentiment Counts ===\")\n",
        "for s, c in sentiment_counts.most_common():\n",
        "    print(f\" {s:<8} {c:,}\")\n",
        "\n",
        "print(\"\\n=== Topics Overview ===\")\n",
        "for t_id, c in topic_counts.most_common():\n",
        "    print(f\"\\n--- Topic {t_id} | {c:,} tweets ---\")\n",
        "    for s, sc in topic_sentiment_counts[t_id].most_common():\n",
        "        print(f\"   {s:<8}: {sc:,}\")\n",
        "\n",
        "    print(\"   Top n-grams:\")\n",
        "    for term, freq in topic_top_terms.get(t_id, []):\n",
        "        print(f\"     {term:<20} {freq}\")\n",
        "\n",
        "    print(\"   Example tweets:\")\n",
        "    for ex in topic_texts[t_id]:\n",
        "        print(\"    -\", ex.replace(\"\\n\", \" \")[:200])\n"
      ],
      "metadata": {
        "id": "SAwLo4nyL0JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, gzip, json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# Path to merged gzipped JSONL\n",
        "DATA_FILE = Path(\"/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/AItrust_pruned_twits_with_sentiment_and_topics_k5.jsonl.gz\")\n",
        "\n",
        "# ‚îÄ‚îÄ Cleaning helpers for TF-IDF ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "URL_RE   = re.compile(r'https?://\\S+')\n",
        "MENT_RE  = re.compile(r'@\\w+')\n",
        "HASH_RE  = re.compile(r'#(\\w+)')        # keep the word, drop '#'\n",
        "NONALPH  = re.compile(r'[^a-zA-Z]+')    # strip non letters\n",
        "SHORT_RE = re.compile(r'\\b\\w{1,2}\\b')   # drop very short tokens\n",
        "\n",
        "TWITTER_STOP = {\n",
        "    'rt','amp','https','http','tco','via','img','video','tweet','retweet',\n",
        "    'follow','join','click','share','subscribe','like'\n",
        "}\n",
        "\n",
        "def clean_text_basic(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(' ', s)\n",
        "    s = MENT_RE.sub(' ', s)\n",
        "    s = HASH_RE.sub(r' \\1 ', s)         # keep hashtag word\n",
        "    s = NONALPH.sub(' ', s)\n",
        "    s = SHORT_RE.sub(' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def tokenizer(s: str):\n",
        "    # split, drop twitter stopwords\n",
        "    toks = s.split()\n",
        "    return [t for t in toks if t not in TWITTER_STOP]\n",
        "\n",
        "# ‚îÄ‚îÄ Step 1: Stream sample ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "sample_texts = []\n",
        "sample_topics = []\n",
        "sample_sents  = []\n",
        "sample_dists  = []\n",
        "sample_dates  = []\n",
        "\n",
        "MAX_SAMPLE = 500_000  # adjust up/down based on RAM/time\n",
        "with gzip.open(DATA_FILE, \"rt\", encoding=\"utf-8\") as fin:\n",
        "    for i, line in enumerate(fin):\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "        except:\n",
        "            continue\n",
        "        txt = obj.get(\"text\", \"\")\n",
        "        if not txt:\n",
        "            continue\n",
        "\n",
        "        sample_texts.append(txt)\n",
        "        sample_topics.append(obj.get(\"lda_k5_topic_id\"))\n",
        "        sample_sents.append(obj.get(\"sentiment_label\"))\n",
        "        sample_dists.append(obj.get(\"lda_k5_topic_dist\"))\n",
        "        sample_dates.append(obj.get(\"created_at\",\"\")[:10])  # YYYY-MM-DD\n",
        "\n",
        "        if i + 1 >= MAX_SAMPLE:\n",
        "            break\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(sample_texts):,} tweets for analysis\")\n",
        "\n",
        "# ‚îÄ‚îÄ Step 2: Basic counts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "topic_counts = Counter(sample_topics)\n",
        "print(\"\\n=== Topic counts (argmax assignment) ===\")\n",
        "for t in sorted(topic_counts):\n",
        "    c = topic_counts[t]\n",
        "    print(f\"Topic {t}: {c:,} ({c/len(sample_texts):.1%})\")\n",
        "\n",
        "# ‚îÄ‚îÄ Step 3: TF-IDF distinctive words per topic (with cleaning) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "URL_RE   = re.compile(r'https?://\\S+')\n",
        "MENT_RE  = re.compile(r'@\\w+')\n",
        "HASH_RE  = re.compile(r'#(\\w+)')\n",
        "NONALPH  = re.compile(r'[^a-zA-Z]+')\n",
        "SHORT_RE = re.compile(r'\\b\\w{1,2}\\b')\n",
        "\n",
        "TWITTER_STOP = {\n",
        "    'rt','amp','https','http','tco','via','img','video','tweet','retweet',\n",
        "    'follow','join','click','share','subscribe','like'\n",
        "}\n",
        "# Domain-global words to suppress across topics\n",
        "GLOBAL_STOP = {\n",
        "    'ai','chatgpt','gpt','openai','artificial','intelligence','artificialintelligence',\n",
        "    'google','bard','microsoft','bing','llm','ml','nlp','dataset','model','models'\n",
        "}\n",
        "\n",
        "def clean_text_basic(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(' ', s)\n",
        "    s = MENT_RE.sub(' ', s)\n",
        "    s = HASH_RE.sub(r' \\1 ', s)    # keep hashtag word\n",
        "    s = NONALPH.sub(' ', s)\n",
        "    s = SHORT_RE.sub(' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def tokenizer_clean(s: str):\n",
        "    toks = s.split()\n",
        "    stops = TWITTER_STOP | GLOBAL_STOP\n",
        "    return [t for t in toks if t not in stops]\n",
        "\n",
        "# Vectorizer tuned to drop very common/rare terms and compress TF\n",
        "tfidf = TfidfVectorizer(\n",
        "    preprocessor=clean_text_basic,\n",
        "    tokenizer=tokenizer_clean,\n",
        "    stop_words='english',\n",
        "    max_df=0.20,          # drop terms in >20% of docs (kills global boilerplate)\n",
        "    min_df=50,            # keep terms seen in at least 50 docs (for stability on 500k)\n",
        "    sublinear_tf=True,    # log(1 + tf)\n",
        "    max_features=20_000,\n",
        ")\n",
        "\n",
        "X = tfidf.fit_transform(sample_texts)\n",
        "terms = np.array(tfidf.get_feature_names_out())\n",
        "topics_arr = np.array(sample_topics)\n",
        "\n",
        "topic_top_terms = {}\n",
        "for t in sorted(set(x for x in topics_arr if x is not None)):\n",
        "    mask_in = (topics_arr == t)\n",
        "    mask_out = ~mask_in\n",
        "    if mask_in.sum() == 0 or mask_out.sum() == 0:\n",
        "        continue\n",
        "\n",
        "    mean_in  = X[mask_in].mean(axis=0).A1\n",
        "    mean_out = X[mask_out].mean(axis=0).A1\n",
        "    lift = mean_in - mean_out                # contrastive TF-IDF (‚Äútopic ‚Äì rest‚Äù)\n",
        "\n",
        "    top_idx = np.argsort(lift)[::-1][:15]\n",
        "    topic_top_terms[t] = list(terms[top_idx])\n",
        "\n",
        "print(\"\\n=== Top contrastive TF-IDF terms per topic (cleaned) ===\")\n",
        "for t in sorted(topic_top_terms):\n",
        "    print(f\"Topic {t}: {', '.join(topic_top_terms[t])}\")\n",
        "\n",
        "\n",
        "# --- Step 3b: ngrams per topic ---\n",
        "def top_ngrams(texts, n=15, ngram_range=(1,3)):\n",
        "    vec = CountVectorizer(\n",
        "        stop_words='english',\n",
        "        ngram_range=ngram_range,\n",
        "        max_features=50000,\n",
        "        lowercase=True,\n",
        "    )\n",
        "    X = vec.fit_transform(texts)\n",
        "    counts = np.array(X.sum(axis=0)).ravel()\n",
        "    vocab = np.array(vec.get_feature_names_out())\n",
        "    idx = counts.argsort()[::-1][:n]\n",
        "    return list(zip(vocab[idx], counts[idx]))\n",
        "\n",
        "# --- Step 3c: N-grams per topic ---\n",
        "topic_ngrams = {}\n",
        "for t in sorted(set(sample_topics)):\n",
        "    texts_t = [txt for txt, tid in zip(sample_texts, sample_topics) if tid == t]\n",
        "    topic_ngrams[t] = top_ngrams(texts_t, n=15, ngram_range=(1,3))\n",
        "\n",
        "print(\"\\n=== Top n-grams per topic ===\")\n",
        "for t, grams in topic_ngrams.items():\n",
        "    tops = \", \".join([f\"{g} ({c:,})\" for g,c in grams])\n",
        "    print(f\"Topic {t}: {tops}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bj9RmR_JPiNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOP0a6xObD3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define human-readable topic labels\n",
        "topic_labels = {\n",
        "    0: \"General AI / Tools & Utility\",\n",
        "    1: \"Memes / Culture\",\n",
        "    2: \"Crypto / NFT / Giveaways / Hype\",\n",
        "    3: \"Tech / Research / ML Ethics\",\n",
        "    4: \"ChatGPT vs Google Bard\"\n",
        "}\n",
        "\n",
        "# ‚îÄ‚îÄ Step 5: Sentiment by topic (stacked proportions) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "df = pd.DataFrame({\n",
        "    \"topic\": [topic_labels[t] for t in sample_topics],\n",
        "    \"sentiment\": sample_sents\n",
        "})\n",
        "sent_by_topic = df.groupby([\"topic\",\"sentiment\"]).size().unstack(fill_value=0)\n",
        "(sent_by_topic\n",
        "     .div(sent_by_topic.sum(axis=1), axis=0)\n",
        "     .plot(kind=\"bar\", stacked=True, figsize=(10,5)))\n",
        "plt.title(\"Sentiment distribution by topic (sample)\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "plt.xlabel(\"Topic\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚îÄ‚îÄ Step 6: Timeline by topic (monthly proportions) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "df_time = pd.DataFrame({\n",
        "    \"topic\": [topic_labels[t] for t in sample_topics],\n",
        "    \"date\": pd.to_datetime(sample_dates, errors=\"coerce\")\n",
        "}).dropna(subset=[\"date\"])\n",
        "df_time[\"month\"] = df_time[\"date\"].dt.to_period(\"M\")\n",
        "timeline = df_time.groupby([\"month\",\"topic\"]).size().unstack(fill_value=0)\n",
        "(timeline\n",
        "     .div(timeline.sum(axis=1), axis=0)\n",
        "     .plot(figsize=(12,6)))\n",
        "plt.title(\"Topic prevalence over time (sample)\")\n",
        "plt.ylabel(\"Proportion of tweets\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vyezP-0nUypO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ Step 4: Overlap (conditional) + Heatmap + Topic-Overlap Graph ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Parameters\n",
        "THRESH    = 0.25   # topic membership threshold\n",
        "EDGE_MIN  = 0.25   # only draw edges with conditional overlap > EDGE_MIN\n",
        "ROUND_TO  = 3      # display rounding for the table/heatmap\n",
        "\n",
        "K = len(sample_dists[0]) if sample_dists else 0\n",
        "labels_in_order = [topic_labels[i] for i in range(K)]\n",
        "\n",
        "# Build conditional overlap: P(Tj > THRESH | Ti > THRESH)\n",
        "overlap = np.zeros((K, K), dtype=float)\n",
        "counts_i = np.zeros(K, dtype=float)  # denominator per row i\n",
        "\n",
        "for dist in sample_dists:\n",
        "    # Boolean mask for which topics clear the threshold in THIS tweet\n",
        "    mask = [(d is not None) and (d > THRESH) for d in dist]\n",
        "    for i, mi in enumerate(mask):\n",
        "        if mi:\n",
        "            counts_i[i] += 1\n",
        "            for j, mj in enumerate(mask):\n",
        "                if mj:\n",
        "                    overlap[i, j] += 1\n",
        "\n",
        "# Normalize rows (handle zero denominators and set diagonal conventionally to 1)\n",
        "for i in range(K):\n",
        "    if counts_i[i] > 0:\n",
        "        overlap[i, :] /= counts_i[i]\n",
        "    else:\n",
        "        overlap[i, i] = 1.0\n",
        "\n",
        "df_overlap = pd.DataFrame(overlap, index=labels_in_order, columns=labels_in_order)\n",
        "\n",
        "print(f\"\\n=== Conditional overlap matrix: P(Tj > {THRESH} | Ti > {THRESH}) ===\")\n",
        "display(df_overlap.round(ROUND_TO))\n",
        "\n",
        "# Heatmap (matplotlib, single plot)\n",
        "plt.figure(figsize=(7.5, 6))\n",
        "plt.imshow(df_overlap.values, aspect='auto')  # default colormap\n",
        "plt.colorbar(label=\"Conditional overlap\")\n",
        "plt.xticks(ticks=np.arange(K), labels=labels_in_order, rotation=45, ha='right')\n",
        "plt.yticks(ticks=np.arange(K), labels=labels_in_order)\n",
        "plt.title(f\"Topic Overlap Heatmap  (threshold={THRESH})\")\n",
        "# annotate with values\n",
        "for i in range(K):\n",
        "    for j in range(K):\n",
        "        val = df_overlap.values[i, j]\n",
        "        plt.text(j, i, f\"{val:.2f}\", ha='center', va='center', fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Topic overlap graph (spring layout). Edge i‚Üíj weight = P(Tj | Ti)\n",
        "# We'll draw an undirected edge with weight = max(P(i|j), P(j|i)) for readability,\n",
        "# but annotate width by that max weight so thicker edges = stronger two-way overlap.\n",
        "G = nx.Graph()\n",
        "for i, lab_i in enumerate(labels_in_order):\n",
        "    G.add_node(lab_i)\n",
        "\n",
        "# Build undirected weights from the conditional matrix\n",
        "for i in range(K):\n",
        "    for j in range(i+1, K):\n",
        "        w_ij = overlap[i, j]\n",
        "        w_ji = overlap[j, i]\n",
        "        w = max(w_ij, w_ji)  # symmetric edge weight for drawing\n",
        "        if w > EDGE_MIN:\n",
        "            G.add_edge(labels_in_order[i], labels_in_order[j],\n",
        "                       weight=w, w_ij=w_ij, w_ji=w_ji)\n",
        "\n",
        "pos = nx.spring_layout(G, seed=42)  # deterministic layout\n",
        "weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
        "# scale widths for visibility\n",
        "edge_widths = [4 * (w ** 2) for w in weights]  # emphasize stronger overlaps\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "nx.draw_networkx_nodes(G, pos, node_size=1200)\n",
        "nx.draw_networkx_labels(G, pos, font_size=9)\n",
        "nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.6)\n",
        "# edge labels show both directions (i|j and j|i) rounded\n",
        "edge_labels = {\n",
        "    (u, v): f\"{G[u][v]['w_ij']:.2f}/{G[u][v]['w_ji']:.2f}\"\n",
        "    for (u, v) in G.edges()\n",
        "}\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
        "plt.title(f\"Topic Overlap Graph (edges where overlap > {EDGE_MIN})\")\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "36Cz9YEuZhym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ Step 6b-alt: Sentiment over time per topic (stacked AREA, proportions) ‚îÄ‚îÄ\n",
        "\n",
        "df_time_sent = pd.DataFrame({\n",
        "    \"topic\": [topic_labels[t] for t in sample_topics],\n",
        "    \"date\": pd.to_datetime(sample_dates, errors=\"coerce\"),\n",
        "    \"sentiment\": sample_sents\n",
        "}).dropna(subset=[\"date\"])\n",
        "df_time_sent[\"month\"] = df_time_sent[\"date\"].dt.to_period(\"M\").astype(str)\n",
        "\n",
        "# Count tweets by (month, topic, sentiment)\n",
        "counts_mts = (df_time_sent\n",
        "              .groupby([\"month\", \"topic\", \"sentiment\"])\n",
        "              .size()\n",
        "              .rename(\"n\")\n",
        "              .reset_index())\n",
        "\n",
        "# Convert counts to proportions within (month, topic)\n",
        "props_mts = (counts_mts\n",
        "             .groupby([\"month\",\"topic\"])\n",
        "             .apply(lambda g: g.assign(prop=g[\"n\"]/g[\"n\"].sum()))\n",
        "             .reset_index(drop=True))\n",
        "\n",
        "months_sorted = sorted(props_mts[\"month\"].unique())\n",
        "sent_order = [\"negative\",\"neutral\",\"positive\"]\n",
        "\n",
        "topics_order = list(dict.fromkeys([topic_labels[t] for t in sorted(set(sample_topics))]))\n",
        "\n",
        "for topic_name in topics_order:\n",
        "    sub = props_mts[props_mts[\"topic\"] == topic_name]\n",
        "    pivot = (sub.pivot_table(index=\"month\", columns=\"sentiment\", values=\"prop\", fill_value=0.0)\n",
        "                 .reindex(index=months_sorted, fill_value=0.0))\n",
        "    cols_present = [c for c in sent_order if c in pivot.columns] + [c for c in pivot.columns if c not in sent_order]\n",
        "    pivot = pivot[cols_present]\n",
        "\n",
        "    ax = pivot.plot(kind=\"area\", stacked=True, figsize=(12,5), alpha=0.8)\n",
        "    ax.set_ylim(0,1)\n",
        "    ax.set_ylabel(\"Proportion\")\n",
        "    ax.set_xlabel(\"Month\")\n",
        "    ax.set_title(f\"Sentiment over time ‚Äî {topic_name}\")\n",
        "    ax.legend(title=\"Sentiment\", loc=\"upper right\", ncol=3, frameon=False)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "fWJV4SGZbEp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUiQh4EgiDWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZcHmPjfgiDZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEMPORARY TESTS --"
      ],
      "metadata": {
        "id": "o6_2nficiCNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(DATA_DIR))\n"
      ],
      "metadata": {
        "id": "ItKs_XgwgU8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Quick author-doc training (in-memory, capped) ‚Üí new LDA + vectorizer (K=5)\n",
        "# Keeps hashtags/tickers, de-emphasizes RT boilerplate, light domain stoplist\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from pathlib import Path\n",
        "import gzip, json, re, unicodedata, numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from joblib import dump\n",
        "\n",
        "# --- Paths (match your project) ---\n",
        "BASE       = Path('/content/drive/My Drive/Colab Projects/AI Public Trust')\n",
        "DATA_DIR   = BASE / 'Data Sets' / 'Cleaned Data'\n",
        "MODELS_DIR = BASE / 'Models' / 'Topic Modeling' / 'LDA'\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Input: any merged gz with 'text' works (sentiment-only or enriched)\n",
        "INPUT_GZ = DATA_DIR / 'AItrust_pruned_twits_with_sentiment_and_topics_k5.jsonl.gz'\n",
        "\n",
        "# Outputs: NEW names so nothing gets overwritten\n",
        "RUN_TAG          = 'authorlite_k5'\n",
        "VECT_PATH_NEW    = MODELS_DIR / f'{RUN_TAG}_vectorizer.joblib'\n",
        "MODEL_PATH_NEW   = MODELS_DIR / f'{RUN_TAG}_lda_k5.joblib'\n",
        "META_JSON_NEW    = MODELS_DIR / f'{RUN_TAG}_topics_metadata.json'\n",
        "\n",
        "# --- Small, safe knobs ---\n",
        "MAX_TWEETS_READ        = 2_000_000   # how many tweets to stream for building author docs\n",
        "MAX_AUTHORS            = 150_000     # cap unique authors\n",
        "MIN_TWEETS_PER_AUTHOR  = 3           # need at least this many tweets to keep an author-doc\n",
        "MAX_DOCS_FOR_TRAIN     = 200_000     # cap author-docs used to fit LDA\n",
        "K_TARGET               = 5           # <-- five topics\n",
        "\n",
        "# --- Cleaning (keeps hashtags/tickers; uses original text for RTs when present) ---\n",
        "url_re   = re.compile(r'https?://\\S+')\n",
        "space_re = re.compile(r'\\s+')\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = url_re.sub(' ', s)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = space_re.sub(' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def get_text(obj):\n",
        "    # prefer the referenced original for retweets\n",
        "    txt = obj.get('text') or ''\n",
        "    if obj.get('type') == 'retweeted':\n",
        "        ref = obj.get('referenced_tweets_dictionary') or {}\n",
        "        txt = ref.get('text') or txt\n",
        "    return txt\n",
        "\n",
        "# --- Build author docs (in memory, bounded) ---\n",
        "auth_texts = defaultdict(list)\n",
        "with gzip.open(INPUT_GZ, 'rt', encoding='utf-8') as fin:\n",
        "    for i, line in enumerate(fin, 1):\n",
        "        if i > MAX_TWEETS_READ:\n",
        "            break\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "        aid = str(obj.get('author_id') or '')\n",
        "        if not aid:\n",
        "            continue\n",
        "        txt = get_text(obj)\n",
        "        if not txt or txt.startswith('rt @'):  # drop bare ‚ÄúRT @‚Äù shells\n",
        "            continue\n",
        "        auth_texts[aid].append(normalize_text(txt))\n",
        "        if len(auth_texts) >= MAX_AUTHORS:\n",
        "            # soft cap on #authors; we keep accumulating for existing authors\n",
        "            pass\n",
        "\n",
        "# filter to authors with enough tweets, then cap total docs\n",
        "author_docs = [\" \\n\".join(v) for (a, v) in auth_texts.items() if len(v) >= MIN_TWEETS_PER_AUTHOR]\n",
        "if len(author_docs) > MAX_DOCS_FOR_TRAIN:\n",
        "    author_docs = author_docs[:MAX_DOCS_FOR_TRAIN]\n",
        "\n",
        "print(f\"Author docs for training: {len(author_docs):,} (from ~{len(auth_texts):,} authors)\")\n",
        "\n",
        "# --- Light domain stoplist removal (so generic words don‚Äôt dominate) ---\n",
        "DOMAIN_STOP = {\n",
        "    'ai','chatgpt','gpt','openai','bard','google','bing','microsoft',\n",
        "    'rt','https','http','amp'\n",
        "}\n",
        "def strip_domain_terms(text: str) -> str:\n",
        "    return re.sub(r'\\b(' + '|'.join(map(re.escape, DOMAIN_STOP)) + r')\\b', ' ', text)\n",
        "\n",
        "author_docs = [strip_domain_terms(t) for t in author_docs]\n",
        "\n",
        "# --- Vectorize (keep hashtags/tickers, include bigrams) ---\n",
        "vect = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words='english',\n",
        "    token_pattern=r\"(?u)\\b[#@$]?[a-zA-Z0-9_]{2,}\\b\",  # keep #aiart $BTC etc.\n",
        "    ngram_range=(1,2),\n",
        "    min_df=5,\n",
        "    max_features=75_000\n",
        ")\n",
        "X = vect.fit_transform(author_docs)\n",
        "print(\"Vocab size:\", len(vect.get_feature_names_out()))\n",
        "\n",
        "# --- LDA fit (batch is stable for topic quality) ---\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=K_TARGET,\n",
        "    learning_method='batch',\n",
        "    learning_decay=0.9,\n",
        "    random_state=42,\n",
        "    max_iter=30,\n",
        "    evaluate_every=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lda.fit(X)\n",
        "print(\"LDA trained for K=5.\")\n",
        "\n",
        "# --- Save artifacts ---\n",
        "dump(vect, VECT_PATH_NEW)\n",
        "dump(lda,  MODEL_PATH_NEW)\n",
        "\n",
        "# --- Quick topic preview & metadata (no heuristic labels) ---\n",
        "import json\n",
        "terms = np.array(vect.get_feature_names_out())\n",
        "TOPN  = 20\n",
        "topics_meta = []\n",
        "for t in range(K_TARGET):\n",
        "    comp = lda.components_[t]\n",
        "    top_idx = comp.argsort()[::-1][:TOPN]\n",
        "    top_terms = terms[top_idx].tolist()\n",
        "    topics_meta.append({\"topic_id\": t, \"top_terms\": top_terms})\n",
        "\n",
        "with open(META_JSON_NEW, 'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        \"k\": K_TARGET,\n",
        "        \"vectorizer_params\": {\n",
        "            \"token_pattern\": r\"(?u)\\b[#@$]?[a-zA-Z0-9_]{2,}\\b\",\n",
        "            \"ngram_range\": [1,2],\n",
        "            \"min_df\": 5,\n",
        "            \"max_features\": 75_000,\n",
        "            \"stop_words\": \"english\"\n",
        "        },\n",
        "        \"domain_stop\": sorted(DOMAIN_STOP),\n",
        "        \"topics\": topics_meta\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Saved: {VECT_PATH_NEW.name}, {MODEL_PATH_NEW.name}, {META_JSON_NEW.name}\")\n",
        "print(\"\\nPreview (top terms only):\")\n",
        "for m in topics_meta:\n",
        "    print(f\"Topic {m['topic_id']:>2}: \" + \", \".join(m['top_terms'][:12]))\n",
        "\n"
      ],
      "metadata": {
        "id": "dSgwIphVluFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 10. Enrich tweets with author-trained topics (safe, batched, non-overwriting)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import os, re, json, time, gzip, io, math, unicodedata\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from joblib import load\n",
        "from collections import Counter\n",
        "\n",
        "# ‚îÄ‚îÄ Paths (match Ignacio layout) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "BASE       = Path('/content/drive/My Drive/Colab Projects/AI Public Trust')\n",
        "DATA_DIR   = BASE / 'Data Sets' / 'Cleaned Data'\n",
        "MODELS_DIR = BASE / 'Models' / 'Topic Modeling' / 'LDA'\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# INPUT can be the big merged gz or plain jsonl (auto-detected)\n",
        "# Use the fully merged file you produced yesterday:\n",
        "PREFERRED_INPUTS = [\n",
        "    DATA_DIR / 'AItrust_pruned_twits_with_sentiment_and_topics_k5.jsonl.gz',  # merged+gz\n",
        "    DATA_DIR / 'AItrust_pruned_twits_with_sentiment_and_topics_k5.jsonl',     # merged (plain)\n",
        "    DATA_DIR / 'AItrust_pruned_twits_with_sentiment.jsonl.gz',                # pre-topic (gz)\n",
        "    DATA_DIR / 'AItrust_pruned_twits_with_sentiment.jsonl',                   # pre-topic (plain)\n",
        "]\n",
        "\n",
        "INPUT_PATH = None\n",
        "for cand in PREFERRED_INPUTS:\n",
        "    if cand.exists():\n",
        "        INPUT_PATH = cand\n",
        "        break\n",
        "if INPUT_PATH is None:\n",
        "    raise FileNotFoundError(\"No input file found. Expected one of: \" + \", \".join(map(str, PREFERRED_INPUTS)))\n",
        "\n",
        "# ‚îÄ‚îÄ Which author model to use (from the quick author-doc training cell) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "RUN_TAG        = 'authorlite'   # keep in sync with your training cell\n",
        "K_TARGET       = 15             # keep in sync with your training cell\n",
        "VECT_PATH_NEW  = MODELS_DIR / f'{RUN_TAG}_vectorizer.joblib'\n",
        "MODEL_PATH_NEW = MODELS_DIR / f'{RUN_TAG}_lda_k{K_TARGET}.joblib'\n",
        "META_JSON_NEW  = MODELS_DIR / f'{RUN_TAG}_k{K_TARGET}_topics_metadata.json'\n",
        "\n",
        "# Sanity: make sure artifacts exist\n",
        "for p in [VECT_PATH_NEW, MODEL_PATH_NEW, META_JSON_NEW]:\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Missing model artifact: {p.name}. \"\n",
        "                                f\"Run the author-doc training cell first.\")\n",
        "\n",
        "# ‚îÄ‚îÄ Outputs (SAFE: include run tag so you never overwrite prior outputs) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "OUT_STEM       = f'AItrust_with_author_topics_{RUN_TAG}_k{K_TARGET}'\n",
        "OUTPUT_JSONL   = DATA_DIR / f'{OUT_STEM}.jsonl'           # final (optional merge target)\n",
        "BLOCK_BASENAME = OUT_STEM + '_block'                      # block prefix\n",
        "WRITE_BLOCKS   = True\n",
        "READ_CHUNK     = 200_000                                  # how many lines per block file\n",
        "BATCH_SIZE     = 50_000                                   # vectorize/transform in batches\n",
        "\n",
        "# ‚îÄ‚îÄ Minimal normalization (match trainer) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "url_re   = re.compile(r'https?://\\S+')\n",
        "space_re = re.compile(r'\\s+')\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = url_re.sub(' ', s)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = space_re.sub(' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def get_text(obj):\n",
        "    # prefer the referenced original for retweets\n",
        "    txt = obj.get('text') or ''\n",
        "    if obj.get('type') == 'retweeted':\n",
        "        ref = obj.get('referenced_tweets_dictionary') or {}\n",
        "        txt = ref.get('text') or txt\n",
        "    return txt\n",
        "\n",
        "# ‚îÄ‚îÄ Load model artifacts & labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "vect = load(VECT_PATH_NEW)\n",
        "lda  = load(MODEL_PATH_NEW)\n",
        "\n",
        "with open(META_JSON_NEW, 'r', encoding='utf-8') as f:\n",
        "    meta = json.load(f)\n",
        "topic_labels = {m['topic_id']: m.get('label', f'Topic {m[\"topic_id\"]}') for m in meta.get('topics', [])}\n",
        "# Fallback if labels missing\n",
        "for t in range(K_TARGET):\n",
        "    topic_labels.setdefault(t, f'Topic {t}')\n",
        "\n",
        "# ‚îÄ‚îÄ Optional: clear old blocks from this RUN_TAG (safe) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "for old in DATA_DIR.glob(BLOCK_BASENAME + \"*.json\"):\n",
        "    try:\n",
        "        old.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ‚îÄ‚îÄ Reader (auto-detect .gz) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def open_any(path: Path):\n",
        "    if str(path).endswith('.gz'):\n",
        "        return gzip.open(path, 'rt', encoding='utf-8', errors='ignore')\n",
        "    return open(path, 'r', encoding='utf-8', errors='ignore')\n",
        "\n",
        "# ‚îÄ‚îÄ Batched enrichment ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(f\"Input : {INPUT_PATH.name}\")\n",
        "print(f\"Model : {MODEL_PATH_NEW.name}\")\n",
        "print(f\"Vector: {VECT_PATH_NEW.name}\")\n",
        "print(f\"Write : blocks={WRITE_BLOCKS}  chunk={READ_CHUNK:,}  batch={BATCH_SIZE:,}\")\n",
        "print(f\"Out   : stem={OUT_STEM}\")\n",
        "\n",
        "start_time      = time.time()\n",
        "processed_total = 0\n",
        "written_total   = 0\n",
        "block_idx       = 1\n",
        "topic_counter   = Counter()\n",
        "\n",
        "buffer_lines    = []      # for block flush\n",
        "buffer_json     = []      # for vectorization batch\n",
        "\n",
        "def flush_block():\n",
        "    \"\"\"Flush buffered lines to next block file\"\"\"\n",
        "    global block_idx, written_total, buffer_lines\n",
        "    if not buffer_lines:\n",
        "        return\n",
        "    block_path = DATA_DIR / f\"{BLOCK_BASENAME}{block_idx:03d}.json\"\n",
        "    with open(block_path, 'w', encoding='utf-8') as fout:\n",
        "        fout.write(\"\\n\".join(buffer_lines) + \"\\n\")\n",
        "    print(f\"Wrote block {block_idx:03d} with {len(buffer_lines):,} lines ‚Üí {block_path.name}\")\n",
        "    written_total += len(buffer_lines)\n",
        "    buffer_lines = []\n",
        "    block_idx += 1\n",
        "\n",
        "def process_batch(objs):\n",
        "    \"\"\"Vectorize & topic-score a batch of tweet objects; return list of enriched JSON strings.\"\"\"\n",
        "    if not objs:\n",
        "        return []\n",
        "    # Prepare texts\n",
        "    texts = [normalize_text(get_text(o) or \"\") for o in objs]\n",
        "    # Vectorize & transform\n",
        "    X = vect.transform(texts)\n",
        "    D = lda.transform(X)  # (batch_size, K)\n",
        "    out_lines = []\n",
        "    for o, dist in zip(objs, D):\n",
        "        tid = int(np.argmax(dist))\n",
        "        label = topic_labels.get(tid, f\"Topic {tid}\")\n",
        "        # Add new fields (do not remove any existing)\n",
        "        o[f'lda_{RUN_TAG}_k{K_TARGET}_topic_id']    = tid\n",
        "        o[f'lda_{RUN_TAG}_k{K_TARGET}_topic_label'] = label\n",
        "        o[f'lda_{RUN_TAG}_k{K_TARGET}_topic_dist']  = [float(x) for x in dist]\n",
        "        topic_counter[tid] += 1\n",
        "        out_lines.append(json.dumps(o, ensure_ascii=False))\n",
        "    return out_lines\n",
        "\n",
        "# ‚îÄ‚îÄ Stream, batch, write ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "with open_any(INPUT_PATH) as fin:\n",
        "    # if not writing blocks, open the single output file (still SAFE filename)\n",
        "    out_main = None if WRITE_BLOCKS else open(OUTPUT_JSONL, 'w', encoding='utf-8')\n",
        "\n",
        "    try:\n",
        "        for line in fin:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "            buffer_json.append(obj)\n",
        "            processed_total += 1\n",
        "\n",
        "            # Process in batches\n",
        "            if len(buffer_json) >= BATCH_SIZE:\n",
        "                enriched = process_batch(buffer_json)\n",
        "                buffer_json = []\n",
        "                if out_main:\n",
        "                    out_main.write(\"\\n\".join(enriched) + \"\\n\")\n",
        "                    written_total += len(enriched)\n",
        "                else:\n",
        "                    buffer_lines.extend(enriched)\n",
        "                    if len(buffer_lines) >= READ_CHUNK:\n",
        "                        flush_block()\n",
        "\n",
        "            # progress\n",
        "            if processed_total % 200_000 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = processed_total / max(elapsed, 1e-9)\n",
        "                print(f\"Progress: {processed_total:,} processed | {rate:,.0f}/s | {int(elapsed)}s elapsed\")\n",
        "\n",
        "        # tail batch\n",
        "        if buffer_json:\n",
        "            enriched = process_batch(buffer_json)\n",
        "            buffer_json = []\n",
        "            if out_main:\n",
        "                out_main.write(\"\\n\".join(enriched) + \"\\n\")\n",
        "                written_total += len(enriched)\n",
        "            else:\n",
        "                buffer_lines.extend(enriched)\n",
        "\n",
        "        # final flush\n",
        "        if out_main:\n",
        "            out_main.flush()\n",
        "            out_main.close()\n",
        "        else:\n",
        "            flush_block()\n",
        "\n",
        "    except Exception as e:\n",
        "        # ensure we close on error\n",
        "        if out_main and not out_main.closed:\n",
        "            out_main.close()\n",
        "        raise e\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Author-topic enrichment done.\")\n",
        "print(f\"Processed: {processed_total:,} | Written: {written_total:,} | Elapsed: {int(elapsed)}s\")\n",
        "print(\"Counts by topic_id:\")\n",
        "for tid in range(K_TARGET):\n",
        "    print(f\"  {tid:2d} ({topic_labels[tid]:25}): {topic_counter.get(tid,0):,}\")\n",
        "\n",
        "print(\"\\nNote:\")\n",
        "print(f\"‚Ä¢ Outputs saved as blocks named {BLOCK_BASENAME}###.json in: {DATA_DIR}\")\n",
        "print(f\"‚Ä¢ This did NOT overwrite the  previous k=5 fields; it added new keys prefixed with lda_{RUN_TAG}_k{K_TARGET}_‚Ä¶\")\n"
      ],
      "metadata": {
        "id": "f16RLAuXpDVV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
