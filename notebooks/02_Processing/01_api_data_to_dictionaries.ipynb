{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Packages"
      ],
      "metadata": {
        "id": "g2AHAGOVpmak"
      },
      "id": "g2AHAGOVpmak"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_switch_setup"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# --- REPO ROOT ON sys.path (so `from src.*` works locally) ---\n",
        "_REPO_ROOT = str(Path(os.getcwd()).resolve().parents[1])\n",
        "if _REPO_ROOT not in sys.path:\n",
        "    sys.path.insert(0, _REPO_ROOT)\n",
        "\n",
        "\n",
        "# --- ENVIRONMENT SWITCH ---\n",
        "# Set to True if running on local machine with Google Drive Desktop mounted\n",
        "# Set to False if running in Google Colab cloud\n",
        "RUNNING_LOCALLY = True\n",
        "\n",
        "if RUNNING_LOCALLY:\n",
        "    # Standard macOS path for Google Drive Desktop\n",
        "    BASE_PATH = Path('/Volumes/GoogleDrive/MyDrive/AI Public Trust')\n",
        "else:\n",
        "    # Google Colab cloud path\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/AI Public Trust')\n",
        "\n",
        "# Pre-compute critical paths used across notebooks\n",
        "twits_folder = BASE_PATH / 'Raw Data/Twits/'\n",
        "test_folder = BASE_PATH / 'Raw Data/'\n",
        "datasets_folder = BASE_PATH / 'Data Sets'\n",
        "cleanedds_folder = BASE_PATH / 'Data Sets/Cleaned Data'\n",
        "networks_folder = BASE_PATH / 'Data Sets/Networks/'\n",
        "literature_folder = BASE_PATH / 'Literature/'\n",
        "topic_models_folder = BASE_PATH / 'Models/Topic Modeling/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb36bf69",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755692574502,
          "user_tz": -120,
          "elapsed": 45665,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "9d34feb9-1556-464a-eb63-5ecf6eed469e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current Directory: /content/drive/My Drive/AI Public Trust/Raw Data/Twits\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "#from tweepy import Stream\n",
        "from datetime import timedelta\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "from datetime import timedelta\n",
        "import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# %%\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1. Drive Mount & Paths\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "# Base project folder (Ignacio standard)\n",
        "# BASE = Path('/content/drive/My Drive/Colab Projects/AI Public Trust')\n",
        "# BASE = Path('/content/drive/My Drive/AI Public Trust')\n",
        "\n",
        "\n",
        "# twits_folder = BASE / 'Raw Data/Twits/'\n",
        "# test_folder = BASE / 'Raw Data/'\n",
        "# print(\"Current Directory:\", twits_folder)\n",
        "# datasets_folder = BASE / 'Data Sets/'\n",
        "# cleanedds_folder = BASE / 'Data Sets' / 'Cleaned Data'"
      ],
      "id": "eb36bf69"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhND4G82hpLf"
      },
      "source": [
        "# Sanity Check"
      ],
      "id": "mhND4G82hpLf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "executionInfo": {
          "elapsed": 24544,
          "status": "ok",
          "timestamp": 1755692599045,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          },
          "user_tz": -120
        },
        "id": "74317c8e",
        "outputId": "70768298-6de7-4ebf-ecf3-c59dca38708a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5016\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tweets_2023-02-06T16:30:00.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# LOOKING AT THE HEAD OF THE FILES TO SEE THEIR SHAPE AND STATS\n",
        "file_list = os.listdir(twits_folder)\n",
        "print(len(file_list))\n",
        "file_list[0]"
      ],
      "id": "74317c8e"
    },
    {
      "cell_type": "code",
      "source": [
        "file_list[:5]"
      ],
      "metadata": {
        "id": "x2fkI1tceVwG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755692599052,
          "user_tz": -120,
          "elapsed": 6,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c07ae42-4cf3-46df-b7ed-c6317ff94446"
      },
      "id": "x2fkI1tceVwG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tweets_2023-02-06T16:30:00.json',\n",
              " 'tweets_2023-02-06T17:00:00.json',\n",
              " 'tweets_2023-02-06T17:30:00.json',\n",
              " 'tweets_2023-02-06T18:00:00.json',\n",
              " 'tweets_2023-02-06T18:30:00.json']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_list[-5:]"
      ],
      "metadata": {
        "id": "DqMywsZNeeLt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755692599071,
          "user_tz": -120,
          "elapsed": 18,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29695a89-ab1f-4e06-fea3-b8e068f108db"
      },
      "id": "DqMywsZNeeLt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tweets_2022-11-15T05:30:00.json',\n",
              " 'tweets_2022-11-15T06:00:00.json',\n",
              " 'tweets_2022-11-15T06:30:00.json',\n",
              " 'tweets_2022-11-15T07:00:00.json',\n",
              " 'tweets_2022-11-15T07:30:00.json']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 1254,
          "status": "ok",
          "timestamp": 1755692600348,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          },
          "user_tz": -120
        },
        "id": "e601fcc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd048894-6b0a-454d-85d8-765142e64cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remember we set the requests limit to 2000.\n",
            "While the total requests found were of 3.\n",
            "Also we set the max per request at 500, and the data has more or less 472 twits per request.\n",
            "--------------- exploring the data ------------\n",
            "<class 'list'>\n",
            "472\n",
            "<class 'dict'>\n",
            "dict_keys(['id', 'referenced_tweets', 'text', 'conversation_id', 'author_id', 'edit_history_tweet_ids', 'public_metrics', 'entities', 'lang', 'possibly_sensitive', 'created_at'])\n",
            "RT @WonderBotz: GENIE automatically captures all user activity and utilizes one-of-a-kind AI to create process flows and actionable insight‚Ä¶\n",
            "2022-11-15T00:09:59.000Z\n",
            "[{'type': 'retweeted', 'id': '1592178048002326529'}]\n",
            "--------------- exploring the extensions ------------\n",
            "<class 'dict'>\n",
            "dict_keys(['users', 'tweets'])\n",
            "<class 'list'>\n",
            "238\n",
            "dict_keys(['id', 'entities', 'text', 'conversation_id', 'author_id', 'edit_history_tweet_ids', 'public_metrics', 'lang', 'possibly_sensitive', 'created_at'])\n",
            "GENIE automatically captures all user activity and utilizes one-of-a-kind AI to create process flows and actionable insights.\n",
            "\n",
            "GENIE requires minimal user interference to derive its full value. https://t.co/jhWKAn8eCd\n",
            "<class 'list'>\n",
            "560\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified'])\n",
            "{'followers_count': 38, 'following_count': 361, 'tweet_count': 210, 'listed_count': 2}\n",
            "--------------- exploring the meta ------------\n",
            "{'newest_id': '1592308899633561601', 'oldest_id': '1592307691929550848', 'result_count': 472, 'next_token': 'b26v89c19zqg8o3fpzhk245f7m9d9ae443n1ouw26gdx9'}\n"
          ]
        }
      ],
      "source": [
        "f = open(test_folder/\"testing.json\",'r',encoding='utf-8')\n",
        "test = json.load(f)\n",
        "f.close()\n",
        "print('Remember we set the requests limit to '+str(2000)+'.')\n",
        "print('While the total requests found were of '+str(len(test))+'.')\n",
        "print('Also we set the max per request at 500, and the data has more or less '+str(len(test[0]['data']))+' twits per request'+'.')\n",
        "\n",
        "print('--------------- exploring the data ------------')\n",
        "page=test[0]\n",
        "data = page['data']\n",
        "print(type(data))\n",
        "print(len(data))\n",
        "print(type(data[0]))\n",
        "print(data[0].keys())\n",
        "print(data[0]['text'])\n",
        "print(data[0]['created_at'])\n",
        "print(data[0]['referenced_tweets'])\n",
        "\n",
        "print('--------------- exploring the extensions ------------')\n",
        "includes = page['includes']\n",
        "print(type(includes))\n",
        "print(includes.keys())\n",
        "print(type(includes['tweets']))\n",
        "print(len(includes['tweets']))\n",
        "print(includes['tweets'][0].keys())\n",
        "print(includes['tweets'][0]['text'])\n",
        "print(type(includes['users']))\n",
        "print(len(includes['users']))\n",
        "print(includes['users'][0].keys())\n",
        "print(includes['users'][0]['public_metrics'])\n",
        "print('--------------- exploring the meta ------------')\n",
        "meta = page['meta']\n",
        "print(meta)"
      ],
      "id": "e601fcc0"
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'tweets_2023-02-07T17:00:00.json'\n",
        "f = open(twits_folder/file,'r',encoding='utf-8')\n",
        "twitfile = json.load(f)\n",
        "f.close()\n",
        "\n",
        "print('Remember we set the requests limit to '+str(2000)+'.')\n",
        "print('While the total requests found were of '+str(len(twitfile))+'.')\n",
        "print('Also we set the max per request at 500, and the data has more or less '+str(len(test[0]['data']))+' twits per request'+'.')\n",
        "print('\\n')\n",
        "print('--------------- exploring the data ------------')\n",
        "page=twitfile[0]\n",
        "data = page['data']\n",
        "print(type(data))\n",
        "print(len(data))\n",
        "print(type(data[0]))\n",
        "print(data[0].keys())\n",
        "print(data[0]['text'])\n",
        "print(data[0]['created_at'])\n",
        "print(data[0]['referenced_tweets'])\n",
        "print(data[0]['public_metrics'])\n",
        "print(data[0]['entities'])\n",
        "\n",
        "print('\\n')\n",
        "print('--------------- exploring the extensions ------------')\n",
        "includes = page['includes']\n",
        "print(type(includes))\n",
        "print(includes.keys())\n",
        "print(type(includes['tweets']))\n",
        "print(len(includes['tweets']))\n",
        "print(includes['tweets'][0].keys())\n",
        "print(includes['tweets'][0]['text'])\n",
        "\n",
        "print(type(includes['users']))\n",
        "print(len(includes['users']))\n",
        "print(includes['users'][0].keys())\n",
        "print(includes['users'][0]['public_metrics'])\n",
        "print(includes['users'][0]['name'])\n",
        "print('\\n')\n",
        "print('--------------- exploring the meta ------------')\n",
        "meta = page['meta']\n",
        "print(meta)"
      ],
      "metadata": {
        "id": "z6jMRA-uOVQL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755692603201,
          "user_tz": -120,
          "elapsed": 2839,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83307937-a60c-47f2-9e2b-c9be0de06ce2"
      },
      "id": "z6jMRA-uOVQL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remember we set the requests limit to 2000.\n",
            "While the total requests found were of 20.\n",
            "Also we set the max per request at 500, and the data has more or less 472 twits per request.\n",
            "\n",
            "\n",
            "--------------- exploring the data ------------\n",
            "<class 'list'>\n",
            "470\n",
            "<class 'dict'>\n",
            "dict_keys(['entities', 'referenced_tweets', 'text', 'lang', 'edit_history_tweet_ids', 'public_metrics', 'conversation_id', 'possibly_sensitive', 'author_id', 'id', 'created_at'])\n",
            "RT @lporiginalg: \"nothing forever\" a 24-hour twitch live stream that is AI generated Seinfeld was banned after calling transgenders mentall‚Ä¶\n",
            "2023-02-07T17:29:59.000Z\n",
            "[{'type': 'retweeted', 'id': '1623004469683425281'}]\n",
            "{'retweet_count': 124, 'reply_count': 0, 'like_count': 0, 'quote_count': 0, 'impression_count': 0}\n",
            "{'annotations': [{'start': 45, 'end': 50, 'probability': 0.8822, 'type': 'Other', 'normalized_text': 'twitch'}, {'start': 85, 'end': 92, 'probability': 0.5293, 'type': 'Person', 'normalized_text': 'Seinfeld'}], 'mentions': [{'start': 3, 'end': 15, 'username': 'lporiginalg', 'id': '110835004'}]}\n",
            "\n",
            "\n",
            "--------------- exploring the extensions ------------\n",
            "<class 'dict'>\n",
            "dict_keys(['users', 'tweets'])\n",
            "<class 'list'>\n",
            "297\n",
            "dict_keys(['entities', 'text', 'lang', 'edit_history_tweet_ids', 'public_metrics', 'conversation_id', 'possibly_sensitive', 'author_id', 'id', 'created_at'])\n",
            "\"nothing forever\" a 24-hour twitch live stream that is AI generated Seinfeld was banned after calling transgenders mentally ill. https://t.co/uCouXd86Ws\n",
            "<class 'list'>\n",
            "660\n",
            "dict_keys(['id', 'description', 'username', 'verified', 'created_at', 'name', 'public_metrics', 'location'])\n",
            "{'followers_count': 76, 'following_count': 645, 'tweet_count': 6854, 'listed_count': 1}\n",
            "Lughüêçüêí\n",
            "\n",
            "\n",
            "--------------- exploring the meta ------------\n",
            "{'newest_id': '1623011204821131264', 'oldest_id': '1623010784891772929', 'result_count': 470, 'next_token': 'b26v89c19zqg8o3fqk70dnnbio851kygzzfdl51w99g1p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tweet Processing Functions"
      ],
      "metadata": {
        "id": "x3qaveoATIWi"
      },
      "id": "x3qaveoATIWi"
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(original_tweet):\n",
        "    # Now we proceed to generate the dictionary and save data\n",
        "    # we create a dictionary 'd' and put the twit data there, which we will json dump and print at the end\n",
        "    twit_dict=dict()\n",
        "    twit_dict['id'] = original_tweet['id']\n",
        "    twit_dict['text']=original_tweet['text']\n",
        "    twit_dict['created_at']=original_tweet['created_at']\n",
        "    twit_dict['public_metrics']=original_tweet['public_metrics']\n",
        "    twit_dict['author_id']=original_tweet['author_id']\n",
        "    twit_dict['type']='original'\n",
        "    try:\n",
        "      twit_dict['type']=original_tweet['referenced_tweets'][0]['type']\n",
        "      twit_dict['referenced_tweets']=original_tweet['referenced_tweets'][0]['id']\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      twit_dict['conversation_id']=original_tweet['conversation_id']\n",
        "    except:\n",
        "      pass\n",
        "    return twit_dict"
      ],
      "metadata": {
        "id": "uIEHjD68VKa9"
      },
      "id": "uIEHjD68VKa9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Data Procesing"
      ],
      "metadata": {
        "id": "0krHgjmypuN0"
      },
      "id": "0krHgjmypuN0"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "AItrust_twits_dict_test = open(cleanedds_folder/'AItrust_twits_dict_test.json','w',encoding='utf-8')\n",
        "AItrust_author_dict_test = open(cleanedds_folder/'AItrust_author_dict_test.json','w',encoding='utf-8')\n",
        "count_processed = 0\n",
        "count_total = 0\n",
        "count_authors = 0\n",
        "count_processed_authors = 0\n",
        "for file in tqdm.tqdm([\"testing.json\"]):\n",
        "    f = open(test_folder/file,'r',encoding='utf-8')\n",
        "    twitfile = json.load(f)\n",
        "    f.close()\n",
        "    for twlist in twitfile:\n",
        "\n",
        "        # Generate dictionary of tweets in extensions in that file\n",
        "        file_twit_extension_dict=dict()\n",
        "        try:\n",
        "          for extension_tweet in twlist['includes']['tweets']:\n",
        "              try:\n",
        "                  processed_tweet = process_tweet(extension_tweet)\n",
        "                  file_twit_extension_dict[processed_tweet['id']]=processed_tweet\n",
        "              except Exception as e:\n",
        "                  print(e)\n",
        "                  pass\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          print('\\n')\n",
        "          print('No tweet extensions.')\n",
        "          print('This happens in file: ' + str(file))\n",
        "          print('\\n')\n",
        "          pass\n",
        "\n",
        "        # Now look at the original tweets and add the extension if useful\n",
        "        file_twit_dict=dict()\n",
        "        for original_tweet in twlist['data']:\n",
        "            count_total+=1\n",
        "            try:\n",
        "                processed_tweet = process_tweet(original_tweet)\n",
        "                file_twit_dict[processed_tweet['id']]=processed_tweet\n",
        "                try:\n",
        "                    referenced_tw_id = processed_tweet['referenced_tweets']\n",
        "                    if referenced_tw_id in file_twit_extension_dict.keys():\n",
        "                        referenced_tw = file_twit_extension_dict[referenced_tw_id]\n",
        "                        processed_tweet['referenced_tweets_dictionary']=referenced_tw\n",
        "                    else:\n",
        "                        processed_tweet['referenced_tweets_dictionary']='N/A'\n",
        "                except:\n",
        "                    pass\n",
        "                dictionary = json.dumps(processed_tweet,ensure_ascii=False)#.encode('utf8')\n",
        "                print(dictionary,file=AItrust_twits_dict_test)\n",
        "                count_processed+=1\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "        # Now see how much you can get from the extensions\n",
        "            try:\n",
        "              for extension_tweet in twlist['includes']['tweets']:\n",
        "                  count_total+=1\n",
        "                  try:\n",
        "                      processed_tweet = process_tweet(extension_tweet)\n",
        "                      try:\n",
        "                          referenced_tw_id = processed_tweet['referenced_tweets']\n",
        "                          if referenced_tw_id in file_twit_extension_dict.keys():\n",
        "                              referenced_tw = file_twit_extension_dict[referenced_tw_id]\n",
        "                              processed_tweet['referenced_tweets_dictionary']=referenced_tw\n",
        "                          if referenced_tw_id in file_twit_dict.keys():\n",
        "                              referenced_tw = file_twit_dict[referenced_tw_id]\n",
        "                              processed_tweet['referenced_tweets_dictionary']=referenced_tw\n",
        "                          else:\n",
        "                              processed_tweet['referenced_tweets_dictionary']='N/A'\n",
        "                      except:\n",
        "                          pass\n",
        "                      dictionary = json.dumps(processed_tweet,ensure_ascii=False)#.encode('utf8')\n",
        "                      print(dictionary,file=AItrust_twits_dict_test)\n",
        "                      count_processed+=1\n",
        "                  except Exception as e:\n",
        "                      print(e)\n",
        "                      pass\n",
        "            except Exception as e:\n",
        "              print(e)\n",
        "              pass\n",
        "\n",
        "        # Now the author\n",
        "        try:\n",
        "          for author in twlist['includes']['users']:\n",
        "            # # dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified'])\n",
        "              count_authors+=1\n",
        "              try:\n",
        "                  author_dictionary = json.dumps(author,ensure_ascii=False)#.encode('utf8')\n",
        "                  print(author_dictionary,file=AItrust_author_dict_test)\n",
        "                  count_processed_authors+=1\n",
        "              except Exception as e:\n",
        "                  print(e)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          print('\\n')\n",
        "          print('No Authors?!!')\n",
        "          print('This happens in file: ' + str(file))\n",
        "          print('\\n')\n",
        "          pass\n",
        "\n",
        "\n",
        "AItrust_twits_dict_test.close()\n",
        "AItrust_author_dict_test.close()\n",
        "print('\\n')\n",
        "print('Tweets counts:')\n",
        "print(count_processed)\n",
        "print(count_total)\n",
        "print('\\n')\n",
        "print('Author counts:')\n",
        "print(count_processed_authors)\n",
        "print(count_authors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjvu-8wRWCsw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755692612861,
          "user_tz": -120,
          "elapsed": 9632,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "2c43956d-4042-46ff-9f20-7014824cc258"
      },
      "id": "Fjvu-8wRWCsw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tweets counts:\n",
            "233094\n",
            "233094\n",
            "\n",
            "\n",
            "Author counts:\n",
            "1272\n",
            "1272\n",
            "CPU times: user 3.38 s, sys: 117 ms, total: 3.5 s\n",
            "Wall time: 9.64 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how it looks, LINE BY LINE\n",
        "# CHECKING FOR TYPES OF TWEET THAT ARE NOT ORIGINAL\n",
        "AItrust_twits_dict_test = open(cleanedds_folder/'AItrust_twits_dict_test.json','r',encoding='utf-8')\n",
        "\n",
        "i = 0\n",
        "for line in AItrust_twits_dict_test:\n",
        "    twit = json.loads(line)\n",
        "    #print(type(twit))\n",
        "    #print(twit['text'])\n",
        "    #print(twit['id'])\n",
        "    #print(twit['type'])\n",
        "    try:\n",
        "      i+=1\n",
        "      print(twit['referenced_tweets'])\n",
        "      print(twit['type'])\n",
        "      print(twit.keys())\n",
        "      print('------')\n",
        "    except:\n",
        "      pass\n",
        "    #print('------')\n",
        "    if i>5:\n",
        "        break\n",
        "AItrust_twits_dict_test.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS30Cr9tldTP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755692612943,
          "user_tz": -120,
          "elapsed": 79,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "8d6ac54e-9d6b-4860-ae4d-a16aa9e5e96d"
      },
      "id": "PS30Cr9tldTP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1592178048002326529\n",
            "retweeted\n",
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'referenced_tweets', 'conversation_id', 'referenced_tweets_dictionary'])\n",
            "------\n",
            "1592187343351463936\n",
            "replied_to\n",
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'referenced_tweets', 'conversation_id', 'referenced_tweets_dictionary'])\n",
            "------\n",
            "1591860196737175553\n",
            "replied_to\n",
            "dict_keys(['id', 'text', 'created_at', 'public_metrics', 'author_id', 'type', 'referenced_tweets', 'conversation_id', 'referenced_tweets_dictionary'])\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how it looks, LINE BY LINE\n",
        "# dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified'])\n",
        "AItrust_author_dict_test = open(cleanedds_folder/'AItrust_author_dict_test.json','r',encoding='utf-8')\n",
        "\n",
        "i = 0\n",
        "for line in AItrust_author_dict_test:\n",
        "    i+=1\n",
        "    author = json.loads(line)\n",
        "    print(author.keys())\n",
        "    print(author['verified'])\n",
        "    if i>10:\n",
        "        break\n",
        "AItrust_author_dict_test.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIRxqZ_Kxx1y",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755692613003,
          "user_tz": -120,
          "elapsed": 51,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "c705bff9-a433-4028-8e8e-273346aa57da"
      },
      "id": "tIRxqZ_Kxx1y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified', 'location'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'name', 'username', 'verified'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified', 'location'])\n",
            "True\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified', 'location'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'name', 'username', 'verified'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified', 'location'])\n",
            "False\n",
            "dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified', 'location'])\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process All Data"
      ],
      "metadata": {
        "id": "N-zAQMFwxf1K"
      },
      "id": "N-zAQMFwxf1K"
    },
    {
      "cell_type": "code",
      "source": [
        "#%%time\n",
        "AItrust_twits_dict = open(cleanedds_folder/'AItrust_twits_dict.json','w',encoding='utf-8')\n",
        "AItrust_author_dict = open(cleanedds_folder/'AItrust_author_dict.json','w',encoding='utf-8')\n",
        "count_processed = 0\n",
        "count_total = 0\n",
        "count_authors = 0\n",
        "count_processed_authors = 0\n",
        "\n",
        "for file in tqdm.tqdm(file_list):\n",
        "    f = open(twits_folder/file,'r',encoding='utf-8')\n",
        "    twitfile = json.load(f)\n",
        "    f.close()\n",
        "    for twlist in twitfile:\n",
        "\n",
        "        # Generate dictionary of tweets in extensions in that file\n",
        "        try:\n",
        "            file_twit_extension_dict=dict()\n",
        "            for extension_tweet in twlist['includes']['tweets']:\n",
        "                try:\n",
        "                    processed_tweet = process_tweet(extension_tweet)\n",
        "                    file_twit_extension_dict[processed_tweet['id']]=processed_tweet\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "                    pass\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "# Now look at the original tweets and add the extension if useful\n",
        "        try:\n",
        "            file_twit_dict=dict()\n",
        "            for original_tweet in twlist['data']:\n",
        "                count_total+=1\n",
        "                try:\n",
        "                    processed_tweet = process_tweet(original_tweet)\n",
        "                    file_twit_dict[processed_tweet['id']]=processed_tweet\n",
        "                    try:\n",
        "                        referenced_tw_id = processed_tweet['referenced_tweets']\n",
        "                        if referenced_tw_id in file_twit_extension_dict.keys():\n",
        "                            referenced_tw = file_twit_extension_dict[referenced_tw_id]\n",
        "                            processed_tweet['referenced_tweets_dictionary']=referenced_tw\n",
        "                        else:\n",
        "                            processed_tweet['referenced_tweets_dictionary']='N/A'\n",
        "                    except:\n",
        "                        pass\n",
        "                    dictionary = json.dumps(processed_tweet,ensure_ascii=False)#.encode('utf8')\n",
        "                    print(dictionary,file=AItrust_twits_dict)\n",
        "                    count_processed+=1\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "                    pass\n",
        "        except Exception as e:\n",
        "            print('\\n')\n",
        "            print('No data!')\n",
        "            print('This happens in file: ' + str(file))\n",
        "            print('This is error: ' + str(e))\n",
        "            print('\\n')\n",
        "            pass\n",
        "\n",
        "\n",
        "        # Now see how much you can get from the extensions\n",
        "        try:\n",
        "            for extension_tweet in twlist['includes']['tweets']:\n",
        "                count_total+=1\n",
        "                try:\n",
        "                    processed_tweet = process_tweet(extension_tweet)\n",
        "                    try:\n",
        "                        referenced_tw_id = processed_tweet['referenced_tweets']\n",
        "                        if referenced_tw_id in file_twit_extension_dict.keys():\n",
        "                            referenced_tw = file_twit_extension_dict[referenced_tw_id]\n",
        "                            processed_tweet['referenced_tweets_dictionary']=referenced_tw\n",
        "                        if referenced_tw_id in file_twit_dict.keys():\n",
        "                            referenced_tw = file_twit_dict[referenced_tw_id]\n",
        "                            processed_tweet['referenced_tweets_dictionary']=referenced_tw\n",
        "                        else:\n",
        "                            processed_tweet['referenced_tweets_dictionary']='N/A'\n",
        "                    except:\n",
        "                        pass\n",
        "                    dictionary = json.dumps(processed_tweet,ensure_ascii=False)#.encode('utf8')\n",
        "                    print(dictionary,file=AItrust_twits_dict)\n",
        "                    count_processed+=1\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "                    pass\n",
        "        except Exception as e:\n",
        "            print('\\n')\n",
        "            print('No tweet extensions.')\n",
        "            print('This happens in file: ' + str(file))\n",
        "            print('This is error: ' + str(e))\n",
        "            print('\\n')\n",
        "            pass\n",
        "\n",
        "        # Now Authors\n",
        "        try:\n",
        "            for author in twlist['includes']['users']:\n",
        "                # # dict_keys(['description', 'public_metrics', 'created_at', 'id', 'entities', 'name', 'username', 'verified'])\n",
        "                count_authors+=1\n",
        "                try:\n",
        "                    author_dictionary = json.dumps(author,ensure_ascii=False)#.encode('utf8')\n",
        "                    print(author_dictionary,file=AItrust_author_dict)\n",
        "                    count_processed_authors+=1\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "        except Exception as e:\n",
        "            print('\\n')\n",
        "            print('No Authors?!!')\n",
        "            print('This happens in file: ' + str(file))\n",
        "            print('This is error: ' + str(e))\n",
        "            print('\\n')\n",
        "            pass\n",
        "\n",
        "AItrust_twits_dict.close()\n",
        "AItrust_author_dict.close()\n",
        "\n",
        "print('\\n')\n",
        "print('Tweets counts:')\n",
        "print(count_processed)\n",
        "print(count_total)\n",
        "print('\\n')\n",
        "print('Author counts:')\n",
        "print(count_processed_authors)\n",
        "print(count_authors)"
      ],
      "metadata": {
        "id": "Ys60fWscxfEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca102f2-2b94-4356-aff6-0668a8f65e8d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755702463516,
          "user_tz": -120,
          "elapsed": 9850512,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        }
      },
      "id": "Ys60fWscxfEs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|‚ñå         | 257/5016 [09:54<2:25:57,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-02-12T00:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|‚ñâ         | 448/5016 [16:38<2:45:22,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-02-16T00:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|‚ñà‚ñç        | 736/5016 [27:51<2:32:11,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No data!\n",
            "This happens in file: tweets_2022-12-31T23:30:00.json\n",
            "This is error: 'data'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-31T23:30:00.json\n",
            "This is error: 'includes'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "No Authors?!!\n",
            "This happens in file: tweets_2022-12-31T23:30:00.json\n",
            "This is error: 'includes'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|‚ñà‚ñã        | 845/5016 [31:55<2:13:03,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-02-24T06:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|‚ñà‚ñã        | 847/5016 [31:59<2:18:30,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-02-24T07:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|‚ñà‚ñã        | 849/5016 [32:03<2:10:30,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-02-24T08:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|‚ñà‚ñà‚ñè       | 1074/5016 [39:44<1:55:51,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-01-18T09:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|‚ñà‚ñà‚ñé       | 1134/5016 [41:42<2:15:54,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-01-19T15:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 1264/5016 [45:57<1:55:41,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-01-22T08:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|‚ñà‚ñà‚ñà       | 1516/5016 [54:33<2:09:04,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-01-27T14:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|‚ñà‚ñà‚ñà‚ñä      | 1885/5016 [1:08:12<2:02:07,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-02-04T06:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|‚ñà‚ñà‚ñà‚ñä      | 1912/5016 [1:09:20<1:53:17,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-02-04T20:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|‚ñà‚ñà‚ñà‚ñà      | 2031/5016 [1:13:19<1:18:05,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-27T15:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 2087/5016 [1:15:01<1:46:48,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-28T19:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 2156/5016 [1:16:56<1:28:48,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-30T05:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2677/5016 [1:32:42<1:14:44,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-01-10T02:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2723/5016 [1:34:17<1:09:39,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-01-11T01:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2745/5016 [1:34:56<1:04:23,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2023-01-11T12:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3245/5016 [1:51:59<50:06,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-11T06:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3258/5016 [1:52:21<54:13,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-11T12:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3266/5016 [1:52:36<54:31,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-11T16:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3486/5016 [2:00:13<44:56,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-16T07:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3540/5016 [2:02:08<38:51,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-17T09:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3855/5016 [2:11:28<54:26,  2.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-23T23:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3942/5016 [2:13:45<28:46,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-25T18:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4220/5016 [2:20:54<19:31,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-11-19T21:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4258/5016 [2:21:54<30:12,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-11-20T16:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 4364/5016 [2:24:40<19:10,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-11-22T21:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4503/5016 [2:28:14<13:47,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-11-25T19:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4522/5016 [2:28:42<11:24,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-11-26T04:30:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4801/5016 [2:36:19<07:23,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "No tweet extensions.\n",
            "This happens in file: tweets_2022-12-02T00:00:00.json\n",
            "This is error: 'tweets'\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5016/5016 [2:44:10<00:00,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tweets counts:\n",
            "36560405\n",
            "36560405\n",
            "\n",
            "\n",
            "Author counts:\n",
            "31989322\n",
            "31989322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnect from Runtime"
      ],
      "metadata": {
        "id": "rEt1OJum3_LZ"
      },
      "id": "rEt1OJum3_LZ"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Javascript\n",
        "\n",
        "# Get current time in New York\n",
        "nyc_time = datetime.now(pytz.timezone('America/New_York'))\n",
        "formatted_time = nyc_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "\n",
        "# Print and log\n",
        "print(f\"‚úÖ Disconnected from runtime at: {formatted_time}\")\n",
        "\n",
        "# Disconnect Colab runtime\n",
        "display(Javascript('google.colab.kernel.disconnect()'))"
      ],
      "metadata": {
        "id": "JO8I5IbHxfHS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1755702463571,
          "user_tz": -120,
          "elapsed": 49,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42946481-fe3a-4587-abeb-8b08e68c92a1"
      },
      "id": "JO8I5IbHxfHS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Disconnected from runtime at: 2025-08-20 11:07:43 EDT\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.kernel.disconnect()"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hw19opn7xfKH"
      },
      "id": "hw19opn7xfKH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKq8q-kKxfMz"
      },
      "id": "GKq8q-kKxfMz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4daexskUxfPl"
      },
      "id": "4daexskUxfPl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PYAHBoYYxfTC"
      },
      "id": "PYAHBoYYxfTC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVMB3Z8pxfVd"
      },
      "id": "MVMB3Z8pxfVd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZDeN7bX5isi"
      },
      "id": "RZDeN7bX5isi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mjxhslxo5ivb"
      },
      "id": "mjxhslxo5ivb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHfphK5J5iyT"
      },
      "id": "VHfphK5J5iyT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "x3qaveoATIWi",
        "0krHgjmypuN0"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
