{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KhU_WrhSsIZ0"
      ],
      "authorship_tag": "ABX9TyPmk4awGjNYRAmmkt15XKFo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "bHUVH9y4CJRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_switch_setup"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# --- REPO ROOT ON sys.path (so `from src.*` works locally) ---\n",
        "_REPO_ROOT = str(Path(os.getcwd()).resolve().parents[1])\n",
        "if _REPO_ROOT not in sys.path:\n",
        "    sys.path.insert(0, _REPO_ROOT)\n",
        "\n",
        "\n",
        "# --- ENVIRONMENT SWITCH ---\n",
        "# Set to True if running on local machine with Google Drive Desktop mounted\n",
        "# Set to False if running in Google Colab cloud\n",
        "RUNNING_LOCALLY = True\n",
        "\n",
        "if RUNNING_LOCALLY:\n",
        "    # Standard macOS path for Google Drive Desktop\n",
        "    BASE_PATH = Path('/Volumes/GoogleDrive/My Drive/Colab Projects/AI Public Trust')\n",
        "else:\n",
        "    # Google Colab cloud path\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = Path('/content/drive/My Drive/Colab Projects/AI Public Trust')\n",
        "\n",
        "# Pre-compute critical paths used across notebooks\n",
        "twits_folder = BASE_PATH / 'Raw Data/Twits/'\n",
        "test_folder = BASE_PATH / 'Raw Data/'\n",
        "datasets_folder = BASE_PATH / 'Data Sets'\n",
        "cleanedds_folder = BASE_PATH / 'Data Sets/Cleaned Data'\n",
        "networks_folder = BASE_PATH / 'Data Sets/Networks/'\n",
        "literature_folder = BASE_PATH / 'Literature/'\n",
        "topic_models_folder = BASE_PATH / 'Models/Topic Modeling/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Dpwh-f48cP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917311414,
          "user_tz": -120,
          "elapsed": 39673,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "8aad9311-0db9-4c29-d94f-2385dd91f848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current Directory: /content/drive/My Drive/Colab Projects/AI Public Trust/Raw Data/Twits/\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import networkx as nx\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "# Ensure that necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# twits_folder = '/content/drive/MyDrive/AI Public Trust/Raw Data/Twits/'\n",
        "# test_folder = '/content/drive/MyDrive/AI Public Trust/Raw Data/'\n",
        "# print(\"Current Directory:\", twits_folder)\n",
        "# datasets_folder = '/content/drive/MyDrive/AI Public Trust/Data Sets/'\n",
        "# cleanedds_folder = '/content/drive/MyDrive/AI Public Trust/Data Sets/Cleaned Data/'\n",
        "# networks_folder = '/content/drive/MyDrive/AI Public Trust/Data Sets/Networks/'\n",
        "# literature_folder = '/content/drive/MyDrive/AI Public Trust/Literature/'\n",
        "# topic_models_folder = '/content/drive/MyDrive/AI Public Trust/Models/Topic Modeling/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with open(topic_models_folder+\"test_sentences_corpus.pkl\", 'rb') as f:\n",
        "    corpus_dict = pickle.load(f)\n",
        "# Create Corpus\n",
        "test_twit_corpus = []\n",
        "for twid in tqdm.tqdm(corpus_dict):\n",
        "  twit = corpus_dict[twid]\n",
        "  text = twit['text']\n",
        "  test_twit_corpus.append(text)\n",
        "\n",
        "print(len(test_twit_corpus))\n",
        "random.sample(test_twit_corpus,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXoWrpkB7g-F",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917314426,
          "user_tz": -120,
          "elapsed": 756,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "92cd06ef-c60a-4d75-9e11-bd4dccab7b3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 630/630 [00:00<00:00, 744760.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "630\n",
            "CPU times: user 16.6 ms, sys: 1.21 ms, total: 17.8 ms\n",
            "Wall time: 1.08 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['machinelearning steps daysofcode g ai analytics artificialintelligence bigdata cloud coding data datascience github iot javascript linux ml mlops nlp nodejs opensource python sql tscottclendaniel womenwhocode',\n",
              " 'chinese researchers created cyborg pigeon watch end data datascience programming datascientist ai rob']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhU_WrhSsIZ0"
      },
      "source": [
        "# LDA SKLEARN\n",
        "\n",
        "- https://machinelearninggeek.com/latent-dirichlet-allocation-using-scikit-learn/\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7VHgJJYtsLtN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917323156,
          "user_tz": -120,
          "elapsed": 769,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        }
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from joblib import dump, load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 767,
          "status": "ok",
          "timestamp": 1714917325488,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          },
          "user_tz": -120
        },
        "id": "a0d_CpW8sL1S",
        "outputId": "097d77af-adb2-49ba-cc6e-834e8f21f25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 67.1 ms, sys: 1.89 ms, total: 69 ms\n",
            "Wall time: 379 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Colab Projects/AI Public Trust/Models/Topic Modeling/test_tfidf.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "%%time\n",
        "# Initialize regex tokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Vectorize document using TF-IDF\n",
        "tfidf = TfidfVectorizer(lowercase=True,\n",
        "                        #stop_words='english',\n",
        "                        ngram_range = (1,1),\n",
        "                        tokenizer = tokenizer.tokenize)\n",
        "\n",
        "# Fit and Transform the documents\n",
        "train_data = tfidf.fit_transform(test_twit_corpus)\n",
        "\n",
        "# Save\n",
        "dump(tfidf, topic_models_folder+'test_tfidf.joblib')\n",
        "#tfidf_2 = load(topic_models_folder+'test_tfidf.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U7_2qNOsL5d",
        "outputId": "bdac2d6d-401f-49ca-f9cc-7ccec3633ef0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917327727,
          "user_tz": -120,
          "elapsed": 1502,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1 of max_iter: 10\n",
            "iteration: 2 of max_iter: 10\n",
            "iteration: 3 of max_iter: 10\n",
            "iteration: 4 of max_iter: 10\n",
            "iteration: 5 of max_iter: 10\n",
            "iteration: 6 of max_iter: 10\n",
            "iteration: 7 of max_iter: 10\n",
            "iteration: 8 of max_iter: 10\n",
            "iteration: 9 of max_iter: 10\n",
            "iteration: 10 of max_iter: 10\n",
            "CPU times: user 1.13 s, sys: 0 ns, total: 1.13 s\n",
            "Wall time: 1.44 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Define the number of topics or components\n",
        "num_components=5\n",
        "\n",
        "# Create LDA object\n",
        "model=LatentDirichletAllocation(n_components=num_components,verbose=2)\n",
        "\n",
        "# Fit and Transform SVD model on data\n",
        "lda_matrix = model.fit_transform(train_data)\n",
        "\n",
        "#save this\n",
        "dump(lda_matrix, topic_models_folder+'test_lda_matrix.joblib')\n",
        "#lda_matrix_2 = load(topic_models_folder+'test_lda_matrix.joblib')\n",
        "\n",
        "# Get Components\n",
        "lda_components=model.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqEBraynsoS_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917330789,
          "user_tz": -120,
          "elapsed": 3,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "1ef75a73-b490-4ef0-bf18-d106a47428fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0:  ['ai', 'deeplearning', 'v', 'fint', 'theapplication', 'machinelearning', 'datascience']\n",
            "Topic 1:  ['conventions', 'say', 'alley', 'exhibitors', 'sections', 'generated', 'artist']\n",
            "Topic 2:  ['ai', 'python', 'iot', 'bigdata', 'cloud', 'g', 'theyre']\n",
            "Topic 3:  ['ai', 'machinelearning', 'datascience', 'bot', 'amp', 'btc', 'artificialintelligence']\n",
            "Topic 4:  ['ai', 'art', 'calling', 'free', 'watch', 'chinese', 'cyborg']\n"
          ]
        }
      ],
      "source": [
        "# Print the topics with their terms\n",
        "terms = tfidf.get_feature_names_out()\n",
        "\n",
        "for index, component in enumerate(lda_components):\n",
        "    zipped = zip(terms, component)\n",
        "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
        "    top_terms_list=list(dict(top_terms_key).keys())\n",
        "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def do_sklearn_lda(n_comp=5):\n",
        "  # Create LDA object\n",
        "  model=LatentDirichletAllocation(n_components=n_comp,verbose=2)\n",
        "\n",
        "  # Fit and Transform SVD model on data\n",
        "  lda_matrix = model.fit_transform(train_data)\n",
        "\n",
        "  #save this\n",
        "  dump(lda_matrix, topic_models_folder+ str(n_comp)+'_test_lda_matrix.joblib')\n",
        "  lda_matrix_2 = load(topic_models_folder+str(n_comp)+'_test_lda_matrix.joblib')\n",
        "\n",
        "  # Get Components\n",
        "  lda_components=model.components_\n",
        "\n",
        "  # Print the topics with their terms\n",
        "  terms = tfidf.get_feature_names_out()\n",
        "  print('SKLearn LDA with '+ str(n_comp)+' components:')\n",
        "  for index, component in enumerate(lda_components):\n",
        "      zipped = zip(terms, component)\n",
        "      top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
        "      top_terms_list=list(dict(top_terms_key).keys())\n",
        "      print(\"Topic \"+str(index)+\": \",top_terms_list)\n",
        "  print('-----------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9Nj-y5Zi1G3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917333771,
          "user_tz": -120,
          "elapsed": 3,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "a9257141-d667-4b4c-84bb-5849a291448e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 10.7 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for n in [5,10,15,20]:\n",
        "  do_sklearn_lda(n_comp=n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJGpBR8ejnQt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917341330,
          "user_tz": -120,
          "elapsed": 4560,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "5f5c7aa8-d200-48bc-ab5b-260c55657e30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1 of max_iter: 10\n",
            "iteration: 2 of max_iter: 10\n",
            "iteration: 3 of max_iter: 10\n",
            "iteration: 4 of max_iter: 10\n",
            "iteration: 5 of max_iter: 10\n",
            "iteration: 6 of max_iter: 10\n",
            "iteration: 7 of max_iter: 10\n",
            "iteration: 8 of max_iter: 10\n",
            "iteration: 9 of max_iter: 10\n",
            "iteration: 10 of max_iter: 10\n",
            "SKLearn LDA with 5 components:\n",
            "Topic 0:  ['ai', 'neuralink', 'like', 'much', 'rbxs', 'coders', 'taught']\n",
            "Topic 1:  ['generated', 'art', 'conventions', 'alley', 'exhibitors', 'sections', 'say']\n",
            "Topic 2:  ['ai', 'art', 'certification', 'challenge', 'developer', 'optimise', 'smart']\n",
            "Topic 3:  ['ai', 'machinelearning', 'bigdata', 'datascience', 'analytics', 'data', 'python']\n",
            "Topic 4:  ['ai', 'foodspot', 'amp', 'artificialintelligence', 'machinelearning', 'bot', 'ml']\n",
            "-----------------------------------\n",
            "iteration: 1 of max_iter: 10\n",
            "iteration: 2 of max_iter: 10\n",
            "iteration: 3 of max_iter: 10\n",
            "iteration: 4 of max_iter: 10\n",
            "iteration: 5 of max_iter: 10\n",
            "iteration: 6 of max_iter: 10\n",
            "iteration: 7 of max_iter: 10\n",
            "iteration: 8 of max_iter: 10\n",
            "iteration: 9 of max_iter: 10\n",
            "iteration: 10 of max_iter: 10\n",
            "SKLearn LDA with 10 components:\n",
            "Topic 0:  ['ai', 'sometimes', 'art', 'dont', 'girl', 'use', 'phone']\n",
            "Topic 1:  ['ai', 'python', 'datascience', 'free', 'bigdata', 'using', 'machinelearning']\n",
            "Topic 2:  ['alley', 'exhibitors', 'sections', 'conventions', 'say', 'artist', 'art']\n",
            "Topic 3:  ['ai', 'one', 'interested', 'gm', 'like', 'art', 'going']\n",
            "Topic 4:  ['ai', 'calling', 'art', 'tools', 'stop', 'instead', 'python']\n",
            "Topic 5:  ['foodspot', 'ai', 'quickly', 'technologies', 'available', 'dishes', 'empowers']\n",
            "Topic 6:  ['theyre', 'chinese', 'cyborg', 'pigeon', 'ai', 'created', 'rob']\n",
            "Topic 7:  ['ai', 'technology', 'product', 'choice', 'editors', 'machinelearning', 'algorithms']\n",
            "Topic 8:  ['ai', 'art', 'neuralink', 'amp', 'artificial', 'intelligence', 'note']\n",
            "Topic 9:  ['ai', 'analytics', 'data', 'machinelearning', 'deeplearning', 'theapplication', 'datascience']\n",
            "-----------------------------------\n",
            "iteration: 1 of max_iter: 10\n",
            "iteration: 2 of max_iter: 10\n",
            "iteration: 3 of max_iter: 10\n",
            "iteration: 4 of max_iter: 10\n",
            "iteration: 5 of max_iter: 10\n",
            "iteration: 6 of max_iter: 10\n",
            "iteration: 7 of max_iter: 10\n",
            "iteration: 8 of max_iter: 10\n",
            "iteration: 9 of max_iter: 10\n",
            "iteration: 10 of max_iter: 10\n",
            "SKLearn LDA with 15 components:\n",
            "Topic 0:  ['intelligence', 'ai', 'artificial', 'cloud', 'python', 'new', 'documentai']\n",
            "Topic 1:  ['deeplearning', 'theapplication', 'fint', 'v', 'ai', 'analytics', 'data']\n",
            "Topic 2:  ['python', 'bigdata', 'iot', 'datascience', 'foodspot', 'cloud', 'ai']\n",
            "Topic 3:  ['ai', 'im', 'datajobs', 'jobsearch', 'art', 'hiring', 'intoainews']\n",
            "Topic 4:  ['created', 'cyborg', 'pigeon', 'chinese', 'watch', 'rob', 'end']\n",
            "Topic 5:  ['ai', 'ml', 'peer', 'scientists', 'brokeboys', 'brown', 'girls']\n",
            "Topic 6:  ['alley', 'exhibitors', 'sections', 'conventions', 'generated', 'say', 'artist']\n",
            "Topic 7:  ['ways', 'certification', 'challenge', 'developer', 'optimise', 'smart', 'explore']\n",
            "Topic 8:  ['art', 'people', 'go', 'nothing', 'wrong', 'ai', 'hole']\n",
            "Topic 9:  ['bot', 'ai', 'aiml', 'digested', 'elbasheer', 'granola', 'identified']\n",
            "Topic 10:  ['calling', 'theyre', 'ai', 'art', 'images', 'engineers', 'instead']\n",
            "Topic 11:  ['ai', 'art', 'like', 'painting', 'frankensteins', 'monster', 'use']\n",
            "Topic 12:  ['free', 'ai', 'interested', 'gm', 'like', 'identify', 'celebrate']\n",
            "Topic 13:  ['hours', 'btc', 'give', 'neuralink', 'tools', 'save', 'quickly']\n",
            "Topic 14:  ['ai', 'crap', 'space', 'deeplearning', 'artists', 'laugh', 'therapy']\n",
            "-----------------------------------\n",
            "iteration: 1 of max_iter: 10\n",
            "iteration: 2 of max_iter: 10\n",
            "iteration: 3 of max_iter: 10\n",
            "iteration: 4 of max_iter: 10\n",
            "iteration: 5 of max_iter: 10\n",
            "iteration: 6 of max_iter: 10\n",
            "iteration: 7 of max_iter: 10\n",
            "iteration: 8 of max_iter: 10\n",
            "iteration: 9 of max_iter: 10\n",
            "iteration: 10 of max_iter: 10\n",
            "SKLearn LDA with 20 components:\n",
            "Topic 0:  ['save', 'hours', 'tools', 'work', 'ai', 'tomorrow', 'bird']\n",
            "Topic 1:  ['interested', 'gm', 'ai', 'con', 'cons', 'publically', 'shame']\n",
            "Topic 2:  ['art', 'free', 'ai', 'javascript', 'reactjs', 'pytorch', 'iiot']\n",
            "Topic 3:  ['one', 'ai', 'teams', 'im', 'based', 'win', 'cause']\n",
            "Topic 4:  ['foodspot', 'disruptive', 'app', 'technologies', 'available', 'dishes', 'empowers']\n",
            "Topic 5:  ['like', 'ai', 'needs', 'press', 'nojo', 'dodass', 'explain']\n",
            "Topic 6:  ['calling', 'images', 'art', 'instead', 'meaning', 'start', 'requires']\n",
            "Topic 7:  ['amp', 'aijobs', 'hiringnow', 'mljobs', 'art', 'dealers', 'see']\n",
            "Topic 8:  ['datascience', 'programming', 'datascientist', 'researchers', 'cyborg', 'pigeon', 'end']\n",
            "Topic 9:  ['machinelearning', 'cc', 'artificialintelligence', 'ai', 'ml', 'innovation', 'states']\n",
            "Topic 10:  ['bot', 'learn', 'ways', 'ai', 'certification', 'challenge', 'developer']\n",
            "Topic 11:  ['technology', 'algorithms', 'note', 'ai', 'datascience', 'strategic', 'machinelearning']\n",
            "Topic 12:  ['choice', 'editors', 'product', 'honored', 'selected', 'technology', 'ai']\n",
            "Topic 13:  ['digitaltransformation', 'g', 'long', 'cloud', 'ever', 'wonder', 'bigdata']\n",
            "Topic 14:  ['theyre', 'ai', 'engineers', 'tesla', 'autopilot', 'competing', 'outperforming']\n",
            "Topic 15:  ['btc', 'ai', 'ml', 'software', 'ive', 'bigdata', 'algorithms']\n",
            "Topic 16:  ['aiartwork', 'princess', 'rose', 'university', 'play', 'innovative', 'eye']\n",
            "Topic 17:  ['alley', 'exhibitors', 'sections', 'conventions', 'say', 'artist', 'generated']\n",
            "Topic 18:  ['quickly', 'give', 'arise', 'dangerous', 'resultof', 'retreat', 'tasks']\n",
            "Topic 19:  ['theapplication', 'fint', 'v', 'analytics', 'data', 'deeplearning', 'ai']\n",
            "-----------------------------------\n",
            "CPU times: user 3.61 s, sys: 21.5 ms, total: 3.63 s\n",
            "Wall time: 5.02 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU08fTAssoV4"
      },
      "outputs": [],
      "source": [
        "# Coherence\n",
        "# https://stackoverflow.com/questions/60613532/how-do-i-calculate-the-coherence-score-of-an-sklearn-lda-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqCTDPZJKE2L"
      },
      "source": [
        "# Gensim LDA Topic Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rare and common tokens.\n",
        "from gensim.corpora import Dictionary\n",
        "import gensim.corpora as corpora\n",
        "import gensim\n",
        "from pprint import pprint\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "metadata": {
        "id": "nUnXLoy19mmt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917345312,
          "user_tz": -120,
          "elapsed": 749,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gensim_corpora = []\n",
        "for twit in test_twit_corpus:\n",
        "  # Tokenize words\n",
        "  tokens = word_tokenize(twit)\n",
        "  gensim_corpora.append(tokens)\n",
        "random.sample(gensim_corpora,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97xpxqDP-Grj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917348290,
          "user_tz": -120,
          "elapsed": 2,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "4ce7ef61-8a05-4c8f-f242-532a99a7d1b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sometimes',\n",
              "  'ai',\n",
              "  'really',\n",
              "  'bad',\n",
              "  'hands',\n",
              "  'sometimes',\n",
              "  'almost',\n",
              "  'gets',\n",
              "  'right',\n",
              "  'p'],\n",
              " ['overall',\n",
              "  'awareness',\n",
              "  'benefits',\n",
              "  'using',\n",
              "  'ai',\n",
              "  'business',\n",
              "  'artificialintelligence',\n",
              "  'ml',\n",
              "  'machinelearning',\n",
              "  'cc']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o_QptCuaJNxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917349810,
          "user_tz": -120,
          "elapsed": 770,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "942a5bbe-aac7-4f7e-d5f8-c099924260cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 30.9 ms, sys: 3.8 ms, total: 34.7 ms\n",
            "Wall time: 685 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Get Representation\n",
        "id2word = corpora.Dictionary(gensim_corpora)\n",
        "id2word.save(topic_models_folder+\"test_id2word.pkl\")\n",
        "\n",
        "# Term Document Frequency (BOW)\n",
        "bow_test_corpus = [id2word.doc2bow(text) for text in gensim_corpora]\n",
        "\n",
        "with open(topic_models_folder+\"bow_test_corpus.pkl\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(bow_test_corpus, fp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Try Loading\n",
        "with open(topic_models_folder+\"bow_test_corpus.pkl\", \"rb\") as fp:   # Unpickling\n",
        "  bow_test_corpus = pickle.load(fp)\n",
        "\n",
        "id2word = corpora.Dictionary.load(topic_models_folder+\"test_id2word.pkl\")\n",
        "\n",
        "print(len(bow_test_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_Gr1zjS_zhM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917350570,
          "user_tz": -120,
          "elapsed": 5,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        },
        "outputId": "31b96bdd-4fdd-4013-ae70-a758513f59e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "630\n",
            "CPU times: user 2.21 ms, sys: 4 ms, total: 6.22 ms\n",
            "Wall time: 11.9 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ilh71NTkHNdV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714917352072,
          "user_tz": -120,
          "elapsed": 3,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          }
        }
      },
      "outputs": [],
      "source": [
        "def create_topics(num_topics = 5,corpus=bow_test_corpus, id2word=id2word):\n",
        "  # number of topics\n",
        "  #num_topics = 5\n",
        "  # Build LDA model\n",
        "  lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                        id2word=id2word,\n",
        "                                        alpha=0.1,\n",
        "                                        eta=0.1,\n",
        "                                        num_topics=num_topics)\n",
        "\n",
        "  temp_file = topic_models_folder+\"lda_model_\"+ str(num_topics)\n",
        "  lda_model.save(temp_file)\n",
        "\n",
        "  # Print the Keyword in the 5 topics\n",
        "  pprint(lda_model.print_topics())\n",
        "  doc_lda = lda_model[corpus]\n",
        "  #Show first 4 important words in the topics\n",
        "  #doc_lda.show_topics(num_topics, 4)\n",
        "  #pprint(lda_model.show_topics(num_topics, 4))\n",
        "  cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
        "  coherence = cm.get_coherence()\n",
        "  print(coherence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4593,
          "status": "ok",
          "timestamp": 1714917358982,
          "user": {
            "displayName": "Ignacio Ojea",
            "userId": "11425136657122854785"
          },
          "user_tz": -120
        },
        "id": "iXlE9pIoJEpg",
        "outputId": "e7a3e51b-84f3-4f48-8c77-4a939ae4b003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.061*\"ai\" + 0.013*\"art\" + 0.011*\"datascience\" + 0.009*\"data\" + 0.008*\"amp\" '\n",
            "  '+ 0.007*\"free\" + 0.007*\"python\" + 0.007*\"machinelearning\" + '\n",
            "  '0.006*\"programming\" + 0.005*\"ml\"'),\n",
            " (1,\n",
            "  '0.046*\"ai\" + 0.009*\"art\" + 0.008*\"amp\" + 0.007*\"python\" + 0.005*\"iot\" + '\n",
            "  '0.005*\"technology\" + 0.005*\"daysofcode\" + 0.005*\"foodspot\" + '\n",
            "  '0.004*\"javascript\" + 0.004*\"data\"'),\n",
            " (2,\n",
            "  '0.068*\"ai\" + 0.010*\"data\" + 0.009*\"machinelearning\" + 0.006*\"day\" + '\n",
            "  '0.006*\"intelligence\" + 0.006*\"artificial\" + 0.006*\"new\" + 0.005*\"artists\" + '\n",
            "  '0.005*\"art\" + 0.005*\"learn\"'),\n",
            " (3,\n",
            "  '0.058*\"ai\" + 0.035*\"art\" + 0.021*\"generated\" + 0.021*\"artist\" + 0.019*\"say\" '\n",
            "  '+ 0.019*\"exhibitors\" + 0.019*\"alley\" + 0.019*\"conventions\" + '\n",
            "  '0.019*\"sections\" + 0.005*\"make\"'),\n",
            " (4,\n",
            "  '0.057*\"ai\" + 0.013*\"machinelearning\" + 0.011*\"art\" + 0.009*\"datascience\" + '\n",
            "  '0.009*\"bigdata\" + 0.009*\"like\" + 0.007*\"amp\" + '\n",
            "  '0.007*\"artificialintelligence\" + 0.006*\"python\" + 0.005*\"much\"')]\n",
            "-11.82437792895503\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "[(0,\n",
            "  '0.064*\"ai\" + 0.018*\"art\" + 0.017*\"artist\" + 0.014*\"generated\" + 0.013*\"say\" '\n",
            "  '+ 0.012*\"exhibitors\" + 0.012*\"conventions\" + 0.012*\"alley\" + '\n",
            "  '0.012*\"sections\" + 0.007*\"machinelearning\"'),\n",
            " (1,\n",
            "  '0.037*\"ai\" + 0.012*\"machinelearning\" + 0.010*\"datascience\" + 0.009*\"data\" + '\n",
            "  '0.008*\"artificialintelligence\" + 0.008*\"chinese\" + 0.007*\"datascientist\" + '\n",
            "  '0.006*\"pigeon\" + 0.006*\"rob\" + 0.006*\"cyborg\"'),\n",
            " (2,\n",
            "  '0.044*\"ai\" + 0.009*\"foodspot\" + 0.007*\"app\" + 0.006*\"art\" + 0.006*\"amp\" + '\n",
            "  '0.006*\"data\" + 0.006*\"technologies\" + 0.005*\"discover\" + 0.005*\"us\" + '\n",
            "  '0.005*\"datascience\"'),\n",
            " (3,\n",
            "  '0.047*\"ai\" + 0.008*\"intelligence\" + 0.008*\"artificial\" + 0.007*\"like\" + '\n",
            "  '0.005*\"art\" + 0.005*\"amp\" + 0.004*\"new\" + 0.004*\"foodspot\" + '\n",
            "  '0.004*\"favorite\" + 0.004*\"using\"'),\n",
            " (4,\n",
            "  '0.056*\"ai\" + 0.013*\"art\" + 0.006*\"see\" + 0.005*\"work\" + '\n",
            "  '0.005*\"artificialintelligence\" + 0.004*\"business\" + 0.004*\"datascience\" + '\n",
            "  '0.004*\"amp\" + 0.004*\"one\" + 0.004*\"machinelearning\"'),\n",
            " (5,\n",
            "  '0.038*\"ai\" + 0.009*\"much\" + 0.008*\"art\" + 0.008*\"give\" + 0.007*\"data\" + '\n",
            "  '0.007*\"like\" + 0.007*\"quickly\" + 0.006*\"great\" + 0.006*\"btc\" + '\n",
            "  '0.006*\"trading\"'),\n",
            " (6,\n",
            "  '0.054*\"ai\" + 0.009*\"art\" + 0.008*\"amp\" + 0.006*\"new\" + 0.006*\"technology\" + '\n",
            "  '0.005*\"like\" + 0.005*\"data\" + 0.004*\"neuralink\" + 0.004*\"theyre\" + '\n",
            "  '0.004*\"python\"'),\n",
            " (7,\n",
            "  '0.040*\"ai\" + 0.018*\"art\" + 0.007*\"nft\" + 0.005*\"datascience\" + '\n",
            "  '0.005*\"artists\" + 0.005*\"amp\" + 0.005*\"machinelearning\" + 0.004*\"bigdata\" + '\n",
            "  '0.004*\"generated\" + 0.004*\"note\"'),\n",
            " (8,\n",
            "  '0.059*\"ai\" + 0.022*\"art\" + 0.008*\"bigdata\" + 0.008*\"calling\" + '\n",
            "  '0.008*\"technology\" + 0.008*\"machinelearning\" + 0.008*\"datascience\" + '\n",
            "  '0.007*\"generated\" + 0.007*\"using\" + 0.007*\"please\"'),\n",
            " (9,\n",
            "  '0.051*\"ai\" + 0.012*\"machinelearning\" + 0.011*\"python\" + 0.009*\"data\" + '\n",
            "  '0.008*\"art\" + 0.008*\"iot\" + 0.007*\"datascience\" + 0.006*\"javascript\" + '\n",
            "  '0.005*\"amp\" + 0.005*\"ml\"')]\n",
            "-11.60920045463713\n",
            "---------------------------------------------\n",
            "CPU times: user 1.48 s, sys: 114 ms, total: 1.59 s\n",
            "Wall time: 4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for k in [5,10]:\n",
        "  print('---------------------------------------------')\n",
        "  create_topics(num_topics = k,corpus=bow_test_corpus, id2word=id2word)\n",
        "  print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pQqELASA2nw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
